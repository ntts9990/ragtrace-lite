#!/usr/bin/env python3
"""
RAGAS ì§ì ‘ í…ŒìŠ¤íŠ¸ - ê° ë©”íŠ¸ë¦­ë³„ë¡œ ê°œë³„ ì‹¤í–‰
"""
import os
import sys
import json
import asyncio
from datetime import datetime

sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from ragtrace_lite.config_loader import load_config
from ragtrace_lite.llm_factory import create_llm
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall


async def test_single_metric(metric, dataset, llm, embeddings=None):
    """ë‹¨ì¼ ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸"""
    print(f"\n{'='*60}")
    print(f"ğŸ“Š {metric.__class__.__name__} í…ŒìŠ¤íŠ¸")
    print(f"{'='*60}")
    
    try:
        # ë©”íŠ¸ë¦­ ì„¤ì •
        if hasattr(metric, 'llm'):
            metric.llm = llm
        if hasattr(metric, 'embeddings') and embeddings:
            metric.embeddings = embeddings
            
        print(f"â³ í‰ê°€ ì‹œì‘...")
        start_time = datetime.now()
        
        # í‰ê°€ ì‹¤í–‰
        result = evaluate(
            dataset=dataset,
            metrics=[metric],
            llm=llm,
        )
        
        elapsed = (datetime.now() - start_time).total_seconds()
        print(f"âœ… í‰ê°€ ì™„ë£Œ ({elapsed:.1f}ì´ˆ)")
        
        # ê²°ê³¼ ì¶œë ¥
        metric_name = metric.__class__.__name__.lower()
        if metric_name in result.scores:
            score = result.scores[metric_name]
            print(f"ğŸ“ˆ ì ìˆ˜: {score}")
            
            # ìƒì„¸ ê²°ê³¼
            if hasattr(result, 'to_pandas'):
                df = result.to_pandas()
                if metric_name in df.columns:
                    detail_score = df[metric_name].iloc[0]
                    print(f"   ìƒì„¸: {detail_score}")
        else:
            print(f"âŒ ì ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ì ìˆ˜: {list(result.scores.keys())}")
            
    except Exception as e:
        print(f"âŒ í‰ê°€ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()


async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("=" * 80)
    print("ğŸ” RAGAS ì§ì ‘ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    # ì„¤ì • ë° LLM ë¡œë“œ
    config = load_config()
    llm = create_llm(config)
    print(f"âœ… LLM ë¡œë“œ: {type(llm).__name__}")
    
    # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (answer_relevancyì— í•„ìš”)
    from langchain_community.embeddings import HuggingFaceEmbeddings
    embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    print(f"âœ… ì„ë² ë”© ë¡œë“œ: BGE-M3")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_data = {
        'question': ['í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?'],
        'answer': ['í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤. ì„œìš¸ì€ ì•½ 950ë§Œ ëª…ì˜ ì¸êµ¬ê°€ ê±°ì£¼í•˜ëŠ” ëŒ€í•œë¯¼êµ­ ìµœëŒ€ì˜ ë„ì‹œì…ë‹ˆë‹¤.'],
        'contexts': [['ì„œìš¸íŠ¹ë³„ì‹œëŠ” ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì´ì ìµœëŒ€ ë„ì‹œë¡œ, ì •ì¹˜, ê²½ì œ, ë¬¸í™”ì˜ ì¤‘ì‹¬ì§€ì…ë‹ˆë‹¤.']],
        'ground_truths': [['í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì´ë‹¤.']]
    }
    
    dataset = Dataset.from_dict(test_data)
    print(f"âœ… ë°ì´í„°ì…‹ ìƒì„±: {len(dataset)}ê°œ í•­ëª©")
    
    # ê° ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸
    metrics = [
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall
    ]
    
    for metric in metrics:
        await test_single_metric(metric, dataset, llm, embeddings)
    
    print("\n" + "="*80)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("="*80)


if __name__ == "__main__":
    # API í‚¤ ì„¤ì •
    os.environ['CLOVA_STUDIO_API_KEY'] = "nv-d78e840d8f5c4e2faed883a52ea91375gmj8"
    
    # ë¹„ë™ê¸° ì‹¤í–‰
    asyncio.run(main())