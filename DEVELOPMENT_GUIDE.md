# RAGTrace Lite ê°œë°œì ê°€ì´ë“œ

## ëª©ì°¨
1. [ì˜ˆìƒë˜ëŠ” ì—ëŸ¬ íŒ¨í„´ ë° í•´ê²°ë°©ì•ˆ](#1-ì˜ˆìƒë˜ëŠ”-ì—ëŸ¬-íŒ¨í„´-ë°-í•´ê²°ë°©ì•ˆ)
2. [íì‡„ë§ í™˜ê²½ êµ¬ì„± ê°€ì´ë“œ](#2-íì‡„ë§-í™˜ê²½-êµ¬ì„±-ê°€ì´ë“œ)
3. [LLM ëª¨ë¸ êµì²´ ê°€ì´ë“œ](#3-llm-ëª¨ë¸-êµì²´-ê°€ì´ë“œ)
4. [ì„ë² ë”© ëª¨ë¸ êµì²´ ê°€ì´ë“œ](#4-ì„ë² ë”©-ëª¨ë¸-êµì²´-ê°€ì´ë“œ)
5. [ë¬¸ì œ í•´ê²° íŒ](#5-ë¬¸ì œ-í•´ê²°-íŒ)

---

## 1. ì˜ˆìƒë˜ëŠ” ì—ëŸ¬ íŒ¨í„´ ë° í•´ê²°ë°©ì•ˆ

### 1.1 DataFrame Ambiguous Truth Value ì—ëŸ¬

#### ë¬¸ì œ ìƒí™©
Windows í™˜ê²½ì—ì„œ pandas DataFrameì˜ boolean í‰ê°€ ì‹œ ë°œìƒí•˜ëŠ” ì—ëŸ¬ì…ë‹ˆë‹¤.

```python
# ë¬¸ì œê°€ ë˜ëŠ” ì½”ë“œ (main.py:154)
if results_df is None or results_df.empty:
    # ValueError: The truth value of a DataFrame is ambiguous
```

#### í•´ê²° ë°©ë²•
```python
# ë°©ë²• 1: len() ì‚¬ìš© (ê¶Œì¥)
if results_df is None or len(results_df) == 0:
    # ì•ˆì „í•œ ì²˜ë¦¬

# ë°©ë²• 2: shape ì†ì„± ì‚¬ìš©
if results_df is None or results_df.shape[0] == 0:
    # ì•ˆì „í•œ ì²˜ë¦¬

# ë°©ë²• 3: isinstance ì²´í¬ ì¶”ê°€
if results_df is None or (isinstance(results_df, pd.DataFrame) and results_df.empty):
    # ë” ì•ˆì „í•œ ì²˜ë¦¬
```

### 1.2 íƒ€ì… ë³€í™˜ ì—ëŸ¬ (None/NaN ì²˜ë¦¬)

#### ë¬¸ì œ ìƒí™©
ìˆ«ìí˜• ë°ì´í„°ì— Noneì´ë‚˜ ë¬¸ìê°€ ì„ì—¬ ìˆì„ ë•Œ ë°œìƒí•©ë‹ˆë‹¤.

```python
# ë¬¸ì œê°€ ë˜ëŠ” ì½”ë“œ
score = stats.get('average')  # Noneì¼ ìˆ˜ ìˆìŒ
formatted_score = f"{score:.4f}"  # TypeError ë°œìƒ
```

#### í•´ê²° ë°©ë²•
```python
# ë°©ë²• 1: ê¸°ë³¸ê°’ê³¼ í•¨ê»˜ ì‚¬ìš©
score = float(stats.get('average', 0) or 0)  # Noneì´ë©´ 0ìœ¼ë¡œ ë³€í™˜
formatted_score = f"{score:.4f}"

# ë°©ë²• 2: ëª…ì‹œì  íƒ€ì… ì²´í¬
score = stats.get('average')
if score is not None and not pd.isna(score):
    try:
        numeric_score = float(score)
        formatted_score = f"{numeric_score:.4f}"
    except (ValueError, TypeError):
        formatted_score = "N/A"
else:
    formatted_score = "N/A"

# ë°©ë²• 3: pandasì˜ ì•ˆì „í•œ ë³€í™˜ ì‚¬ìš©
numeric_data = pd.to_numeric(data_series, errors='coerce')  # ë³€í™˜ ì‹¤íŒ¨ ì‹œ NaN
numeric_data = numeric_data.fillna(0)  # NaNì„ 0ìœ¼ë¡œ ì±„ìš°ê¸°
```

### 1.3 ê²½ë¡œ ì²˜ë¦¬ ë¬¸ì œ (í¬ë¡œìŠ¤ í”Œë«í¼)

#### ë¬¸ì œ ìƒí™©
Windowsì™€ Unix ì‹œìŠ¤í…œ ê°„ ê²½ë¡œ êµ¬ë¶„ì ì°¨ì´ë¡œ ì¸í•œ ë¬¸ì œì…ë‹ˆë‹¤.

```python
# ë¬¸ì œê°€ ë˜ëŠ” ì½”ë“œ
file_path = "data/input/test.json"  # Windowsì—ì„œ ë¬¸ì œ ê°€ëŠ¥
```

#### í•´ê²° ë°©ë²•
```python
from pathlib import Path

# ë°©ë²• 1: pathlib ì‚¬ìš© (ê¶Œì¥)
file_path = Path("data") / "input" / "test.json"
# ìë™ìœ¼ë¡œ OSì— ë§ëŠ” ê²½ë¡œ êµ¬ë¶„ì ì‚¬ìš©

# ë°©ë²• 2: ì ˆëŒ€ ê²½ë¡œ ë³€í™˜
absolute_path = file_path.absolute()
# Windows: C:\Users\...\data\input\test.json
# Unix: /home/.../data/input/test.json

# ë°©ë²• 3: ë¬¸ìì—´ ë³€í™˜ (ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ìš©)
path_str = str(file_path)  # ì¼ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ë¬¸ìì—´ë§Œ ë°›ìŒ

# ë°©ë²• 4: í”Œë«í¼ ì²´í¬
import platform
if platform.system() == "Windows":
    # Windows íŠ¹ë³„ ì²˜ë¦¬
    pass
```

### 1.4 API Rate Limiting ì²˜ë¦¬

#### ë¬¸ì œ ìƒí™©
API í˜¸ì¶œ ì œí•œìœ¼ë¡œ ì¸í•œ 429 ì—ëŸ¬ ë˜ëŠ” ì—°ê²° ê±°ë¶€ì…ë‹ˆë‹¤.

#### í•´ê²° ë°©ë²•
```python
import time
import asyncio
from typing import Optional

class RateLimiter:
    def __init__(self, min_interval: float = 1.0):
        self.min_interval = min_interval
        self.last_request_time = 0
    
    async def wait_if_needed(self):
        """í•„ìš”ì‹œ ëŒ€ê¸°"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.min_interval:
            wait_time = self.min_interval - time_since_last
            print(f"â±ï¸  Rate limit ëŒ€ê¸°: {wait_time:.1f}ì´ˆ")
            await asyncio.sleep(wait_time)
        
        self.last_request_time = time.time()

# ì‚¬ìš© ì˜ˆì‹œ
rate_limiter = RateLimiter(min_interval=12.0)  # HCXëŠ” 12ì´ˆ

async def call_api():
    await rate_limiter.wait_if_needed()
    # API í˜¸ì¶œ ìˆ˜í–‰
```

---

## 2. íì‡„ë§ í™˜ê²½ êµ¬ì„± ê°€ì´ë“œ

### 2.1 í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

íì‡„ë§ì—ì„œëŠ” ì™¸ë¶€ ë„¤íŠ¸ì›Œí¬ ì ‘ê·¼ì„ ì°¨ë‹¨í•´ì•¼ í•©ë‹ˆë‹¤.

```bash
# Hugging Face ì˜¤í”„ë¼ì¸ ëª¨ë“œ ì„¤ì •
export HF_HUB_OFFLINE=1
export TRANSFORMERS_OFFLINE=1

# í”„ë¡ì‹œ ì„¤ì • (í•„ìš”í•œ ê²½ìš°)
export HTTP_PROXY=http://your-proxy:port
export HTTPS_PROXY=http://your-proxy:port
export NO_PROXY=localhost,127.0.0.1
```

### 2.2 ëª¨ë¸ íŒŒì¼ ì‚¬ì „ ì¤€ë¹„

#### BGE-M3 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì¸í„°ë„· ê°€ëŠ¥í•œ í™˜ê²½ì—ì„œ)
```python
from huggingface_hub import snapshot_download

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
model_path = "./models/bge-m3"
snapshot_download(
    repo_id="BAAI/bge-m3",
    local_dir=model_path,
    local_dir_use_symlinks=False,  # ì¤‘ìš”: ì‹¬ë³¼ë¦­ ë§í¬ ë¹„í™œì„±í™”
    resume_download=True
)

# ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ í™•ì¸
import os
for root, dirs, files in os.walk(model_path):
    for file in files:
        print(os.path.join(root, file))
```

#### ëª¨ë¸ íŒŒì¼ êµ¬ì¡°
```
models/
â””â”€â”€ bge-m3/
    â”œâ”€â”€ config.json
    â”œâ”€â”€ model.safetensors
    â”œâ”€â”€ tokenizer_config.json
    â”œâ”€â”€ tokenizer.json
    â””â”€â”€ special_tokens_map.json
```

### 2.3 ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ì¤€ë¹„

```bash
# 1. ì¸í„°ë„· ê°€ëŠ¥í•œ í™˜ê²½ì—ì„œ íŒ¨í‚¤ì§€ ë‹¤ìš´ë¡œë“œ
pip download -r requirements.txt -d ./offline_packages/

# 2. íì‡„ë§ìœ¼ë¡œ offline_packages í´ë” ì´ë™

# 3. íì‡„ë§ì—ì„œ ì„¤ì¹˜
pip install --no-index --find-links ./offline_packages/ -r requirements.txt
```

---

## 3. LLM ëª¨ë¸ êµì²´ ê°€ì´ë“œ

### 3.1 ìƒˆë¡œìš´ LLM Provider ì¶”ê°€í•˜ê¸°

#### Step 1: Config ì„¤ì • ì¶”ê°€ (config_loader.py)

```python
# config_loader.py ìˆ˜ì •
class LLMConfig(BaseModel):
    """LLM ì„¤ì •"""
    provider: str = Field(..., description="LLM ì œê³µì")
    api_key: Optional[str] = Field(None, description="API í‚¤")
    model_name: Optional[str] = Field(None, description="ëª¨ë¸ ì´ë¦„")
    
    @field_validator('provider')
    @classmethod
    def validate_provider(cls, v):
        # ìƒˆë¡œìš´ provider ì¶”ê°€
        allowed = ['gemini', 'hcx', 'ollama', 'custom_llm']  # ìƒˆ provider ì¶”ê°€
        if v.lower() not in allowed:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ì œê³µì: {v}. í—ˆìš©: {allowed}")
        return v.lower()
```

#### Step 2: LLM Adapter í´ë˜ìŠ¤ êµ¬í˜„

```python
# ìƒˆ íŒŒì¼: custom_llm_adapter.py
class CustomLLMAdapter:
    """ì»¤ìŠ¤í…€ LLM API ì–´ëŒ‘í„°"""
    
    def __init__(self, api_key: str, model_name: str, base_url: str = None):
        self.api_key = api_key
        self.model_name = model_name
        self.base_url = base_url or "http://localhost:8080"
        
        # í—¤ë” ì„¤ì •
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        print(f"ğŸ¤– Custom LLM ì–´ëŒ‘í„° ì´ˆê¸°í™”: {model_name}")
    
    async def agenerate_answer(self, prompt: str, **kwargs) -> str:
        """ë¹„ë™ê¸° API í˜¸ì¶œ"""
        import aiohttp
        
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "temperature": kwargs.get('temperature', 0.7),
            "max_tokens": kwargs.get('max_tokens', 1000),
            # ì¶”ê°€ íŒŒë¼ë¯¸í„°
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/v1/completions",
                    headers=self.headers,
                    json=payload
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data.get('choices', [{}])[0].get('text', '')
                    else:
                        error_text = await response.text()
                        raise Exception(f"API ì—ëŸ¬ ({response.status}): {error_text}")
                        
        except Exception as e:
            print(f"âŒ Custom LLM API ì˜¤ë¥˜: {e}")
            return f"Error: {str(e)}"
    
    def generate_answer(self, prompt: str, **kwargs) -> str:
        """ë™ê¸° í˜¸ì¶œ (ë¹„ë™ê¸°ë¥¼ ë™ê¸°ë¡œ ë˜í•‘)"""
        import asyncio
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìˆìœ¼ë©´ ìŠ¤ë ˆë“œ ì‚¬ìš©
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(
                        asyncio.run, 
                        self.agenerate_answer(prompt, **kwargs)
                    )
                    return future.result()
            else:
                return asyncio.run(self.agenerate_answer(prompt, **kwargs))
        except Exception as e:
            print(f"âŒ Custom LLM ë™ê¸° í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return f"Error: {str(e)}"
```

#### Step 3: LLM Factoryì— í†µí•© (llm_factory.py)

```python
# llm_factory.pyì˜ create_llm í•¨ìˆ˜ ìˆ˜ì •
def create_llm(config: Config) -> LLM:
    """RAGAS í˜¸í™˜ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    provider = config.llm.provider.lower()
    
    try:
        if provider == 'gemini':
            # ê¸°ì¡´ Gemini ì½”ë“œ
            pass
            
        elif provider == 'hcx':
            # ê¸°ì¡´ HCX ì½”ë“œ
            pass
            
        elif provider == 'custom_llm':  # ìƒˆë¡œìš´ provider
            from .custom_llm_adapter import CustomLLMAdapter
            
            # ì„¤ì •ì—ì„œ ì¶”ê°€ íŒŒë¼ë¯¸í„° ì½ê¸°
            base_url = config.llm.get('base_url', 'http://localhost:8080')
            
            adapter = CustomLLMAdapter(
                api_key=config.llm.api_key,
                model_name=config.llm.model_name,
                base_url=base_url
            )
            return LLMAdapterWrapper(adapter)
            
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ì œê³µì: {provider}")
            
    except Exception as e:
        raise Exception(f"LLM ì´ˆê¸°í™” ì‹¤íŒ¨ ({provider}): {str(e)}")
```

#### Step 4: ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸ (config.yaml)

```yaml
# config.yaml
llm:
  provider: custom_llm
  api_key: ${CUSTOM_LLM_API_KEY}
  model_name: "my-custom-model-7b"
  base_url: "http://10.0.0.10:8080"  # íì‡„ë§ ë‚´ë¶€ ì£¼ì†Œ

# ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©
# export CUSTOM_LLM_API_KEY="your-api-key"
```

### 3.2 Ollama í†µí•© ì˜ˆì‹œ (ë¡œì»¬ LLM)

```python
# ollama_adapter.py
class OllamaAdapter:
    """Ollama ë¡œì»¬ LLM ì–´ëŒ‘í„°"""
    
    def __init__(self, model_name: str = "llama2", base_url: str = "http://localhost:11434"):
        self.model_name = model_name
        self.base_url = base_url
        print(f"ğŸ¦™ Ollama ì–´ëŒ‘í„° ì´ˆê¸°í™”: {model_name}")
        
        # ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        self._check_model_exists()
    
    def _check_model_exists(self):
        """ëª¨ë¸ì´ ë¡œì»¬ì— ìˆëŠ”ì§€ í™•ì¸"""
        import requests
        try:
            response = requests.get(f"{self.base_url}/api/tags")
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [m['name'] for m in models]
                if self.model_name not in model_names:
                    print(f"âš ï¸  ëª¨ë¸ '{self.model_name}'ì´ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {model_names}")
        except:
            print("âš ï¸  Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
    
    async def agenerate_answer(self, prompt: str, **kwargs) -> str:
        """Ollama API í˜¸ì¶œ"""
        import aiohttp
        
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": kwargs.get('temperature', 0.7),
                "num_predict": kwargs.get('max_tokens', 1000),
            }
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/api/generate",
                json=payload
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    return data.get('response', '')
                else:
                    raise Exception(f"Ollama API ì—ëŸ¬: {response.status}")
```

---

## 4. ì„ë² ë”© ëª¨ë¸ êµì²´ ê°€ì´ë“œ

### 4.1 ìƒˆë¡œìš´ ì„ë² ë”© Provider ì¶”ê°€í•˜ê¸°

#### Step 1: Config ì„¤ì • ì¶”ê°€ (config_loader.py)

```python
class EmbeddingConfig(BaseModel):
    """ì„ë² ë”© ì„¤ì •"""
    provider: str = Field("default", description="ì„ë² ë”© ì œê³µì")
    device: str = Field("auto", description="ë””ë°”ì´ìŠ¤")
    model_path: Optional[str] = Field(None, description="ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ")
    
    @field_validator('provider')
    @classmethod
    def validate_provider(cls, v):
        # ìƒˆë¡œìš´ provider ì¶”ê°€
        allowed = ['default', 'bge_m3', 'sentence_transformers', 'custom_embedding']
        if v.lower() not in allowed:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„ë² ë”© ì œê³µì: {v}. í—ˆìš©: {allowed}")
        return v.lower()
```

#### Step 2: ì„ë² ë”© ì„¤ì • ë©”ì„œë“œ êµ¬í˜„ (evaluator.py)

```python
# evaluator.pyì— ì¶”ê°€
def _setup_custom_embeddings(self):
    """ì»¤ìŠ¤í…€ ì„ë² ë”© ëª¨ë¸ ì„¤ì •"""
    try:
        from sentence_transformers import SentenceTransformer
        from langchain_huggingface import HuggingFaceEmbeddings
        from ragas.embeddings import LangchainEmbeddingsWrapper
        
        # ì„¤ì •ì—ì„œ ëª¨ë¸ ê²½ë¡œ ì½ê¸°
        model_path = self.config.embedding.model_path or "sentence-transformers/all-MiniLM-L6-v2"
        device = self.config.embedding.device
        
        # ìë™ ë””ë°”ì´ìŠ¤ ì„ íƒ
        if device == 'auto':
            import torch
            if torch.cuda.is_available():
                device = 'cuda'
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                device = 'mps'
            else:
                device = 'cpu'
        
        print(f"ğŸ”§ ì»¤ìŠ¤í…€ ì„ë² ë”© ëª¨ë¸ ë¡œë”©: {model_path} (device: {device})")
        
        # ëª¨ë¸ì´ ë¡œì»¬ì— ìˆëŠ”ì§€ í™•ì¸
        model_path_obj = Path(model_path)
        if model_path_obj.exists():
            print(f"âœ… ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©: {model_path_obj.absolute()}")
            model_name_or_path = str(model_path_obj.absolute())
        else:
            print(f"ğŸ“¥ Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: {model_path}")
            model_name_or_path = model_path
        
        # HuggingFace ì„ë² ë”© ìƒì„±
        lc_embeddings = HuggingFaceEmbeddings(
            model_name=model_name_or_path,
            model_kwargs={'device': device},
            encode_kwargs={'normalize_embeddings': True}  # ì •ê·œí™” ì˜µì…˜
        )
        
        # RAGAS í˜¸í™˜ ë˜í¼ë¡œ ê°ì‹¸ê¸°
        embeddings = LangchainEmbeddingsWrapper(lc_embeddings)
        
        print(f"âœ… ì»¤ìŠ¤í…€ ì„ë² ë”© ë¡œë“œ ì™„ë£Œ (device: {device})")
        return embeddings
        
    except Exception as e:
        raise Exception(f"ì»¤ìŠ¤í…€ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

# _setup_embeddings ë©”ì„œë“œ ìˆ˜ì •
def _setup_embeddings(self):
    """ì„ë² ë”© ëª¨ë¸ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
    embedding_provider = self.config.embedding.provider.lower()
    
    print(f"ğŸ”§ ì„ë² ë”© ì„¤ì •: {embedding_provider}")
    
    if embedding_provider == 'default':
        # ê¸°ì¡´ ì½”ë“œ
        pass
    elif embedding_provider == 'bge_m3':
        # ê¸°ì¡´ ì½”ë“œ
        pass
    elif embedding_provider == 'custom_embedding':  # ìƒˆë¡œìš´ provider
        embeddings = self._setup_custom_embeddings()
        return embeddings
    else:
        print(f"âš ï¸  ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„ë² ë”© ì œê³µì: {embedding_provider}")
        return None
```

### 4.2 ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸ ì˜ˆì‹œ

```python
def _setup_multilingual_embeddings(self):
    """ë‹¤êµ­ì–´ ì§€ì› ì„ë² ë”© ëª¨ë¸ ì„¤ì •"""
    try:
        from langchain_huggingface import HuggingFaceEmbeddings
        from ragas.embeddings import LangchainEmbeddingsWrapper
        
        # ë‹¤êµ­ì–´ ëª¨ë¸ ì˜µì…˜
        multilingual_models = {
            'xlm-roberta': 'sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens',
            'labse': 'sentence-transformers/LaBSE',
            'multilingual-e5': 'intfloat/multilingual-e5-large'
        }
        
        model_name = multilingual_models.get(
            self.config.embedding.get('model_variant', 'xlm-roberta')
        )
        
        print(f"ğŸŒ ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë”©: {model_name}")
        
        lc_embeddings = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs={'device': self.config.embedding.device},
            encode_kwargs={
                'normalize_embeddings': True,
                'batch_size': 32  # ë°°ì¹˜ í¬ê¸° ì¡°ì •
            }
        )
        
        return LangchainEmbeddingsWrapper(lc_embeddings)
        
    except Exception as e:
        raise Exception(f"ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
```

---

## 5. ë¬¸ì œ í•´ê²° íŒ

### 5.1 ë””ë²„ê¹… ë°©ë²•

```python
# ìƒì„¸ ë¡œê·¸ í™œì„±í™”
import logging
logging.basicConfig(level=logging.DEBUG)

# LangChain ë””ë²„ê·¸ ëª¨ë“œ
from langchain.globals import set_debug
set_debug(True)

# í™˜ê²½ ë³€ìˆ˜ë¡œ ë””ë²„ê·¸ í™œì„±í™”
export RAGTRACE_DEBUG=1
```

### 5.2 ì„±ëŠ¥ ìµœì í™”

```python
# ë°°ì¹˜ ì²˜ë¦¬ í™œì„±í™”
config.evaluation.batch_size = 10  # ë™ì‹œ ì²˜ë¦¬ ìˆ˜ ì¦ê°€

# ìºì‹± í™œì„±í™”
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_embedding(text: str):
    return embedding_model.encode(text)
```

### 5.3 ë©”ëª¨ë¦¬ ê´€ë¦¬

```python
# GPU ë©”ëª¨ë¦¬ ì •ë¦¬
import torch
torch.cuda.empty_cache()

# ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
import gc
gc.collect()
```

### 5.4 ì¼ë°˜ì ì¸ ë¬¸ì œì™€ í•´ê²°ì±…

| ë¬¸ì œ | ì›ì¸ | í•´ê²°ì±… |
|------|------|--------|
| "API key not found" | í™˜ê²½ ë³€ìˆ˜ ë¯¸ì„¤ì • | `.env` íŒŒì¼ í™•ì¸ ë˜ëŠ” `export` ëª…ë ¹ ì‚¬ìš© |
| "Model not found" | ëª¨ë¸ íŒŒì¼ ëˆ„ë½ | ëª¨ë¸ ê²½ë¡œ í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ |
| "Out of memory" | GPU/RAM ë¶€ì¡± | ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ ë˜ëŠ” CPU ì‚¬ìš© |
| "Connection timeout" | ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ | í”„ë¡ì‹œ ì„¤ì • í™•ì¸ ë˜ëŠ” íƒ€ì„ì•„ì›ƒ ì¦ê°€ |
| "Rate limit exceeded" | API ì œí•œ ì´ˆê³¼ | Rate limiter ê°„ê²© ì¦ê°€ |

---

## ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- [RAGTrace Lite GitHub](https://github.com/yourusername/ragtrace-lite)
- [RAGAS ê³µì‹ ë¬¸ì„œ](https://docs.ragas.io/)
- [LangChain ë¬¸ì„œ](https://docs.langchain.com/)
- [Hugging Face Hub](https://huggingface.co/)

ì§ˆë¬¸ì´ë‚˜ ë¬¸ì œê°€ ìˆìœ¼ì‹œë©´ ì´ìŠˆë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”!