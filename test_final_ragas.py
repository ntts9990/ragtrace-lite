#!/usr/bin/env python3
"""
ìµœì¢… RAGAS í…ŒìŠ¤íŠ¸
"""
import os
import sys
from datasets import Dataset
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from ragtrace_lite.config_loader import load_config
from ragtrace_lite.llm_factory import create_llm
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall


def test_all_metrics():
    """ëª¨ë“  RAGAS ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸"""
    print("ğŸ¯ ìµœì¢… RAGAS ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    # í™˜ê²½ ì„¤ì •
    os.environ['CLOVA_STUDIO_API_KEY'] = "nv-d78e840d8f5c4e2faed883a52ea91375gmj8"
    config = load_config()
    
    # í”„ë¡ì‹œê°€ ì ìš©ëœ LLM ìƒì„±
    print("\n1ï¸âƒ£ í”„ë¡ì‹œ LLM ìƒì„±")
    llm = create_llm(config)
    print(f"âœ… LLM íƒ€ì…: {type(llm).__name__}")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_data = {
        'question': [
            'í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?',
            'íŒŒì´ì¬ì€ ì–´ë–¤ ì–¸ì–´ì¸ê°€ìš”?'
        ],
        'answer': [
            'í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤. ì„œìš¸ì€ 600ë…„ ì´ìƒì˜ ì—­ì‚¬ë¥¼ ê°€ì§„ ë„ì‹œì…ë‹ˆë‹¤.',
            'íŒŒì´ì¬ì€ ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ìš´ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤. 1991ë…„ì— ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤.'
        ],
        'contexts': [
            ['ì„œìš¸íŠ¹ë³„ì‹œëŠ” ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì´ì ìµœëŒ€ ë„ì‹œì…ë‹ˆë‹¤. ì¡°ì„  ì‹œëŒ€ë¶€í„° ìˆ˜ë„ ì—­í• ì„ í•´ì™”ìŠµë‹ˆë‹¤.'],
            ['íŒŒì´ì¬(Python)ì€ 1991ë…„ ê·€ë„ ë°˜ ë¡œì„¬ì´ ë°œí‘œí•œ ê³ ê¸‰ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¡œ, í”Œë«í¼ì— ë…ë¦½ì ì´ë©° ì¸í„°í”„ë¦¬í„°ì‹, ê°ì²´ì§€í–¥ì  ì–¸ì–´ì…ë‹ˆë‹¤.']
        ],
        'ground_truths': [
            ['ì„œìš¸ì€ í•œêµ­ì˜ ìˆ˜ë„ì´ë‹¤.'],
            ['íŒŒì´ì¬ì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì´ë‹¤.']
        ]
    }
    
    dataset = Dataset.from_dict(test_data)
    print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±: {len(dataset)}ê°œ í•­ëª©")
    
    # ë©”íŠ¸ë¦­ ì„¤ì •
    print("\n2ï¸âƒ£ ë©”íŠ¸ë¦­ ì„¤ì •")
    metrics_to_test = [
        (faithfulness, "Faithfulness"),
        (answer_relevancy, "Answer Relevancy"),
        (context_precision, "Context Precision"),
        (context_recall, "Context Recall")
    ]
    
    # ê° ë©”íŠ¸ë¦­ ê°œë³„ í…ŒìŠ¤íŠ¸
    for metric, name in metrics_to_test:
        print(f"\n{'='*50}")
        print(f"ğŸ“Š {name} í…ŒìŠ¤íŠ¸")
        print(f"{'='*50}")
        
        metric.llm = llm
        
        try:
            result = evaluate(
                dataset=dataset,
                metrics=[metric],
                llm=llm,
                raise_exceptions=False,
                show_progress=False
            )
            
            if hasattr(result, 'to_pandas'):
                df = result.to_pandas()
                metric_col = metric.name
                
                if metric_col in df.columns:
                    scores = df[metric_col].tolist()
                    print(f"âœ… {name} ì ìˆ˜:")
                    for i, score in enumerate(scores):
                        print(f"   ìƒ˜í”Œ {i+1}: {score}")
                    
                    # í‰ê·  ì ìˆ˜ ê³„ì‚° (NaNì´ ì•„ë‹Œ ê°’ë§Œ)
                    valid_scores = [s for s in scores if str(s) != 'nan']
                    if valid_scores:
                        avg_score = sum(valid_scores) / len(valid_scores)
                        print(f"   í‰ê· : {avg_score:.3f}")
                    else:
                        print(f"   âš ï¸ ëª¨ë“  ì ìˆ˜ê°€ NaN")
                else:
                    print(f"âŒ {name} ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
        except Exception as e:
            print(f"âŒ {name} í‰ê°€ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    # ì „ì²´ ë©”íŠ¸ë¦­ ë™ì‹œ í…ŒìŠ¤íŠ¸
    print(f"\n\n{'='*70}")
    print("ğŸ“Š ì „ì²´ ë©”íŠ¸ë¦­ ë™ì‹œ í‰ê°€")
    print(f"{'='*70}")
    
    # ëª¨ë“  ë©”íŠ¸ë¦­ì— LLM ì„¤ì •
    for metric, _ in metrics_to_test:
        metric.llm = llm
    
    try:
        result = evaluate(
            dataset=dataset,
            metrics=[m for m, _ in metrics_to_test],
            llm=llm,
            raise_exceptions=False,
            show_progress=True
        )
        
        print("\nâœ… ì „ì²´ í‰ê°€ ì™„ë£Œ!")
        
        if hasattr(result, 'to_pandas'):
            df = result.to_pandas()
            
            print("\nğŸ“Š ìµœì¢… ê²°ê³¼:")
            print("-" * 50)
            
            for metric, name in metrics_to_test:
                metric_col = metric.name
                if metric_col in df.columns:
                    scores = df[metric_col].tolist()
                    valid_scores = [s for s in scores if str(s) != 'nan']
                    if valid_scores:
                        avg_score = sum(valid_scores) / len(valid_scores)
                        print(f"{name:20s}: {avg_score:.3f}")
                    else:
                        print(f"{name:20s}: NaN")
            
            # ì „ì²´ ë°ì´í„°í”„ë ˆì„ ì¶œë ¥
            print("\nì „ì²´ ë°ì´í„°:")
            print(df)
            
    except Exception as e:
        print(f"\nâŒ ì „ì²´ í‰ê°€ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_all_metrics()