#!/usr/bin/env python3
"""
ìƒì„¸í•œ RAGAS í‰ê°€ í…ŒìŠ¤íŠ¸
ê° ë©”íŠ¸ë¦­ë³„ë¡œ ìƒì„¸ ë¡œê·¸ ì¶œë ¥
"""
import os
import sys
import json
from datetime import datetime

sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from ragtrace_lite.config_loader import load_config
from ragtrace_lite.llm_factory import create_llm
from ragtrace_lite.evaluator import RagasEvaluator
from datasets import Dataset


def test_detailed_evaluation():
    """ê° ë©”íŠ¸ë¦­ë³„ ìƒì„¸ í…ŒìŠ¤íŠ¸"""
    print("=" * 80)
    print("ğŸ” ìƒì„¸ RAGAS í‰ê°€ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    # ì„¤ì • ë¡œë“œ
    config = load_config()
    print(f"âœ… ì„¤ì • ë¡œë“œ: {config.llm.provider}")
    
    # LLM ìƒì„±
    llm = create_llm(config)
    print(f"âœ… LLM ìƒì„±: {type(llm).__name__}")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_data = {
        'question': ['í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?'],
        'answer': ['í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤. ì„œìš¸ì€ ì•½ 950ë§Œ ëª…ì˜ ì¸êµ¬ê°€ ê±°ì£¼í•˜ëŠ” ëŒ€í•œë¯¼êµ­ ìµœëŒ€ì˜ ë„ì‹œì…ë‹ˆë‹¤.'],
        'contexts': [['ì„œìš¸íŠ¹ë³„ì‹œëŠ” ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì´ì ìµœëŒ€ ë„ì‹œë¡œ, ì •ì¹˜, ê²½ì œ, ë¬¸í™”ì˜ ì¤‘ì‹¬ì§€ì…ë‹ˆë‹¤.']],
        'ground_truths': [['í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì´ë‹¤.']]
    }
    
    dataset = Dataset.from_dict(test_data)
    print(f"âœ… ë°ì´í„°ì…‹ ìƒì„±: {len(dataset)}ê°œ í•­ëª©\n")
    
    # ê° ë©”íŠ¸ë¦­ë³„ë¡œ ê°œë³„ í…ŒìŠ¤íŠ¸
    metrics_to_test = ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']
    
    for metric_name in metrics_to_test:
        print(f"\n{'='*60}")
        print(f"ğŸ“Š {metric_name} ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸")
        print(f"{'='*60}")
        
        try:
            # ë‹¨ì¼ ë©”íŠ¸ë¦­ìœ¼ë¡œ í‰ê°€ì ìƒì„±
            evaluator = RagasEvaluator(config, llm=llm, metrics=[metric_name])
            
            # í‰ê°€ ì‹¤í–‰
            print(f"â³ {metric_name} í‰ê°€ ì‹œì‘...")
            start_time = datetime.now()
            
            results_df = evaluator.evaluate(dataset)
            
            elapsed = (datetime.now() - start_time).total_seconds()
            print(f"âœ… {metric_name} í‰ê°€ ì™„ë£Œ ({elapsed:.1f}ì´ˆ)")
            
            # ê²°ê³¼ í™•ì¸
            if metric_name in results_df.columns:
                score = results_df[metric_name].iloc[0]
                print(f"ğŸ“ˆ {metric_name} ì ìˆ˜: {score}")
                
                # NaN ì²´í¬
                import pandas as pd
                if pd.isna(score):
                    print(f"âš ï¸  {metric_name} ì ìˆ˜ê°€ NaNì…ë‹ˆë‹¤!")
                else:
                    print(f"âœ… {metric_name} ì ìˆ˜ ê³„ì‚° ì„±ê³µ: {score:.3f}")
            else:
                print(f"âŒ {metric_name} ì»¬ëŸ¼ì´ ê²°ê³¼ì— ì—†ìŠµë‹ˆë‹¤!")
                print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(results_df.columns)}")
                
        except Exception as e:
            print(f"âŒ {metric_name} í‰ê°€ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    print("\n" + "="*80)
    print("âœ… ìƒì„¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("="*80)


if __name__ == "__main__":
    # API í‚¤ ì„¤ì •
    os.environ['CLOVA_STUDIO_API_KEY'] = "nv-d78e840d8f5c4e2faed883a52ea91375gmj8"
    
    # ìƒì„¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_detailed_evaluation()