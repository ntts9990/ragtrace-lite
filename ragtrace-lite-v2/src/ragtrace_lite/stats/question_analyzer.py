"""Individual question analysis and interpretation"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional
import logging
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class QuestionAnalysis:
    """ê°œë³„ ë¬¸í•­ ë¶„ì„ ê²°ê³¼"""
    question_id: str
    question_text: str
    answer_text: str
    contexts: List[str]
    ground_truth: Optional[str]
    
    # ë©”íŠ¸ë¦­ ì ìˆ˜
    faithfulness: float
    answer_relevancy: float
    context_precision: float
    context_recall: float
    answer_correctness: float
    
    # ë¶„ì„ ê²°ê³¼
    overall_score: float
    status: str  # "good", "warning", "poor"
    issues: List[str]
    recommendations: List[str]
    interpretation: str


class QuestionAnalyzer:
    """ê°œë³„ ë¬¸í•­ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.thresholds = {
            'good': 0.8,
            'warning': 0.6,
            'poor': 0.0
        }
    
    def analyze_question(
        self,
        question_data: Dict[str, Any],
        metrics: Dict[str, float]
    ) -> QuestionAnalysis:
        """
        ê°œë³„ ë¬¸í•­ ë¶„ì„
        
        Args:
            question_data: ë¬¸í•­ ë°ì´í„° (question, answer, contexts, ground_truth)
            metrics: í•´ë‹¹ ë¬¸í•­ì˜ ë©”íŠ¸ë¦­ ì ìˆ˜ë“¤
            
        Returns:
            ë¬¸í•­ ë¶„ì„ ê²°ê³¼
        """
        # ì „ì²´ ì ìˆ˜ ê³„ì‚°
        overall_score = np.mean(list(metrics.values()))
        
        # ìƒíƒœ íŒì •
        status = self._determine_status(overall_score)
        
        # ë¬¸ì œì  ì‹ë³„
        issues = self._identify_issues(metrics)
        
        # ê°œì„  ê¶Œì¥ì‚¬í•­
        recommendations = self._generate_recommendations(metrics, issues)
        
        # ì¢…í•© í•´ì„
        interpretation = self._generate_interpretation(
            question_data, metrics, issues, status
        )
        
        return QuestionAnalysis(
            question_id=question_data.get('question_id', ''),
            question_text=question_data.get('question', ''),
            answer_text=question_data.get('answer', ''),
            contexts=question_data.get('contexts', []),
            ground_truth=question_data.get('ground_truth'),
            faithfulness=metrics.get('faithfulness', 0),
            answer_relevancy=metrics.get('answer_relevancy', 0),
            context_precision=metrics.get('context_precision', 0),
            context_recall=metrics.get('context_recall', 0),
            answer_correctness=metrics.get('answer_correctness', 0),
            overall_score=overall_score,
            status=status,
            issues=issues,
            recommendations=recommendations,
            interpretation=interpretation
        )
    
    def _determine_status(self, score: float) -> str:
        """ì ìˆ˜ì— ë”°ë¥¸ ìƒíƒœ íŒì •"""
        if score >= self.thresholds['good']:
            return 'good'
        elif score >= self.thresholds['warning']:
            return 'warning'
        else:
            return 'poor'
    
    def _identify_issues(self, metrics: Dict[str, float]) -> List[str]:
        """ë¬¸ì œì  ì‹ë³„"""
        issues = []
        
        # ì¶©ì‹¤ë„ ë¬¸ì œ
        if metrics.get('faithfulness', 1) < 0.6:
            issues.append("ë‹µë³€ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë²—ì–´ë‚œ ë‚´ìš© í¬í•¨")
        
        # ê´€ë ¨ì„± ë¬¸ì œ
        if metrics.get('answer_relevancy', 1) < 0.6:
            issues.append("ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì§ì ‘ì ì¸ ê´€ë ¨ì„± ë¶€ì¡±")
        
        # ì»¨í…ìŠ¤íŠ¸ ì •ë°€ë„ ë¬¸ì œ
        if metrics.get('context_precision', 1) < 0.6:
            issues.append("ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì— ë¶ˆí•„ìš”í•œ ì •ë³´ ë§ìŒ")
        
        # ì»¨í…ìŠ¤íŠ¸ ì¬í˜„ìœ¨ ë¬¸ì œ
        if metrics.get('context_recall', 1) < 0.6:
            issues.append("í•„ìš”í•œ ì •ë³´ê°€ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ëˆ„ë½ë¨")
        
        # ì •í™•ë„ ë¬¸ì œ
        if metrics.get('answer_correctness', 1) < 0.5:
            issues.append("ë‹µë³€ì˜ ì‚¬ì‹¤ì  ì •í™•ë„ê°€ ë‚®ìŒ")
        
        # ë³µí•© ë¬¸ì œ ë¶„ì„
        if (metrics.get('context_recall', 1) > 0.8 and 
            metrics.get('answer_correctness', 1) < 0.5):
            issues.append("ì»¨í…ìŠ¤íŠ¸ëŠ” ì¶©ë¶„í•˜ë‚˜ ë‹µë³€ ìƒì„± í’ˆì§ˆ ë¬¸ì œ")
        
        if (metrics.get('context_precision', 1) < 0.5 and
            metrics.get('answer_relevancy', 1) < 0.5):
            issues.append("ê²€ìƒ‰ ë° ìƒì„± ëª¨ë‘ ê°œì„  í•„ìš”")
        
        return issues
    
    def _generate_recommendations(
        self, 
        metrics: Dict[str, float], 
        issues: List[str]
    ) -> List[str]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì¶©ì‹¤ë„ ê°œì„ 
        if metrics.get('faithfulness', 1) < 0.6:
            recommendations.append(
                "í”„ë¡¬í”„íŠ¸ì— 'ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€' ì§€ì‹œ ê°•í™”"
            )
        
        # ê´€ë ¨ì„± ê°œì„ 
        if metrics.get('answer_relevancy', 1) < 0.6:
            recommendations.append(
                "ì§ˆë¬¸ ì´í•´ ê°•í™”ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ í•„ìš”"
            )
        
        # ê²€ìƒ‰ ê°œì„ 
        if metrics.get('context_precision', 1) < 0.6:
            recommendations.append(
                "ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” ë˜ëŠ” ì²­í‚¹ ì „ëµ ì¬ê²€í† "
            )
        
        if metrics.get('context_recall', 1) < 0.6:
            recommendations.append(
                "ì„ë² ë”© ëª¨ë¸ ë³€ê²½ ë˜ëŠ” ê²€ìƒ‰ ê°œìˆ˜(top-k) ì¦ê°€ ê³ ë ¤"
            )
        
        # ì •í™•ë„ ê°œì„ 
        if metrics.get('answer_correctness', 1) < 0.5:
            recommendations.append(
                "LLM íŒŒì¸íŠœë‹ ë˜ëŠ” ë” ê°•ë ¥í•œ ëª¨ë¸ë¡œ ë³€ê²½ ê²€í† "
            )
        
        # ìš°ì„ ìˆœìœ„ ì§€ì •
        if len(recommendations) > 1:
            recommendations.insert(0, f"ìš°ì„ ìˆœìœ„: {self._get_priority_issue(metrics)}")
        
        return recommendations
    
    def _get_priority_issue(self, metrics: Dict[str, float]) -> str:
        """ê°€ì¥ ìš°ì„ ì ìœ¼ë¡œ ê°œì„ í•´ì•¼ í•  ë¬¸ì œ"""
        # ê°€ì¥ ë‚®ì€ ì ìˆ˜ì˜ ë©”íŠ¸ë¦­ ì°¾ê¸°
        min_metric = min(metrics.items(), key=lambda x: x[1])
        
        metric_names = {
            'faithfulness': 'ì¶©ì‹¤ë„',
            'answer_relevancy': 'ë‹µë³€ ê´€ë ¨ì„±',
            'context_precision': 'ì»¨í…ìŠ¤íŠ¸ ì •ë°€ë„',
            'context_recall': 'ì»¨í…ìŠ¤íŠ¸ ì¬í˜„ìœ¨',
            'answer_correctness': 'ë‹µë³€ ì •í™•ë„'
        }
        
        return f"{metric_names.get(min_metric[0], min_metric[0])} ê°œì„  ({min_metric[1]:.2f})"
    
    def _generate_interpretation(
        self,
        question_data: Dict[str, Any],
        metrics: Dict[str, float],
        issues: List[str],
        status: str
    ) -> str:
        """ì¢…í•© í•´ì„ ìƒì„±"""
        
        if status == 'good':
            base_text = "ì´ ë¬¸í•­ì€ ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤."
        elif status == 'warning':
            base_text = "ì´ ë¬¸í•­ì€ ì¼ë¶€ ê°œì„ ì´ í•„ìš”í•œ ìƒíƒœì…ë‹ˆë‹¤."
        else:
            base_text = "ì´ ë¬¸í•­ì€ ì „ë©´ì ì¸ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
        
        # ê²€ìƒ‰ vs ìƒì„± ë¶„ì„
        retrieval_score = np.mean([
            metrics.get('context_precision', 0),
            metrics.get('context_recall', 0)
        ])
        generation_score = np.mean([
            metrics.get('faithfulness', 0),
            metrics.get('answer_relevancy', 0),
            metrics.get('answer_correctness', 0)
        ])
        
        if retrieval_score > generation_score + 0.2:
            analysis = "ê²€ìƒ‰ì€ ì˜ ë˜ì—ˆìœ¼ë‚˜ ë‹µë³€ ìƒì„±ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤."
        elif generation_score > retrieval_score + 0.2:
            analysis = "ë‹µë³€ ìƒì„± ëŠ¥ë ¥ì€ ì¢‹ìœ¼ë‚˜ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ì´ ë¶€ì¡±í•©ë‹ˆë‹¤."
        else:
            analysis = "ê²€ìƒ‰ê³¼ ìƒì„±ì´ ê· í˜•ì¡íŒ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤."
        
        # ì£¼ìš” ë¬¸ì œì 
        if issues:
            issues_text = f"ì£¼ìš” ë¬¸ì œ: {issues[0]}"
        else:
            issues_text = "íŠ¹ë³„í•œ ë¬¸ì œì ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        return f"{base_text} {analysis} {issues_text}"
    
    def batch_analyze(
        self,
        questions_data: List[Dict[str, Any]],
        metrics_list: List[Dict[str, float]]
    ) -> pd.DataFrame:
        """ì—¬ëŸ¬ ë¬¸í•­ ì¼ê´„ ë¶„ì„"""
        
        analyses = []
        for q_data, metrics in zip(questions_data, metrics_list):
            analysis = self.analyze_question(q_data, metrics)
            analyses.append({
                'question': analysis.question_text[:50] + '...' if len(analysis.question_text) > 50 else analysis.question_text,
                'overall_score': analysis.overall_score,
                'status': analysis.status,
                'faithfulness': analysis.faithfulness,
                'answer_relevancy': analysis.answer_relevancy,
                'context_precision': analysis.context_precision,
                'context_recall': analysis.context_recall,
                'answer_correctness': analysis.answer_correctness,
                'main_issue': analysis.issues[0] if analysis.issues else 'None',
                'priority_action': analysis.recommendations[0] if analysis.recommendations else 'None'
            })
        
        df = pd.DataFrame(analyses)
        
        # ìš”ì•½ í†µê³„ ì¶”ê°€
        df['rank'] = df['overall_score'].rank(ascending=False, method='dense')
        
        return df
    
    def identify_patterns(self, analyses_df: pd.DataFrame) -> Dict[str, Any]:
        """ë¬¸í•­ë“¤ ê°„ì˜ íŒ¨í„´ ì‹ë³„"""
        
        patterns = {
            'overall_stats': {
                'mean_score': analyses_df['overall_score'].mean(),
                'std_score': analyses_df['overall_score'].std(),
                'min_score': analyses_df['overall_score'].min(),
                'max_score': analyses_df['overall_score'].max()
            },
            'status_distribution': analyses_df['status'].value_counts().to_dict(),
            'common_issues': {},
            'metric_correlations': {}
        }
        
        # ê³µí†µ ë¬¸ì œì  ë¶„ì„
        issue_counts = {}
        for issues_str in analyses_df['main_issue']:
            if issues_str != 'None':
                issue_counts[issues_str] = issue_counts.get(issues_str, 0) + 1
        
        patterns['common_issues'] = sorted(
            issue_counts.items(), 
            key=lambda x: x[1], 
            reverse=True
        )[:5]
        
        # ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„
        metric_cols = ['faithfulness', 'answer_relevancy', 'context_precision', 
                      'context_recall', 'answer_correctness']
        
        for metric in metric_cols:
            if metric in analyses_df.columns:
                patterns['metric_correlations'][metric] = {
                    'vs_overall': analyses_df[metric].corr(analyses_df['overall_score'])
                }
        
        # ìµœì•…/ìµœê³  ë¬¸í•­ ì‹ë³„
        patterns['worst_questions'] = analyses_df.nsmallest(3, 'overall_score')[
            ['question', 'overall_score', 'main_issue']
        ].to_dict('records')
        
        patterns['best_questions'] = analyses_df.nlargest(3, 'overall_score')[
            ['question', 'overall_score']
        ].to_dict('records')
        
        return patterns
    
    def generate_improvement_plan(self, patterns: Dict[str, Any]) -> List[str]:
        """íŒ¨í„´ ê¸°ë°˜ ê°œì„  ê³„íš ìƒì„±"""
        
        plan = []
        
        # ì „ì²´ ì„±ëŠ¥ ê¸°ë°˜
        mean_score = patterns['overall_stats']['mean_score']
        if mean_score < 0.5:
            plan.append("ğŸš¨ ê¸´ê¸‰: ì „ì²´ì ì¸ ì‹œìŠ¤í…œ ì¬ê²€í†  í•„ìš”")
        elif mean_score < 0.7:
            plan.append("âš ï¸ ì£¼ì˜: ë‹¨ê³„ì  ê°œì„  ì „ëµ ìˆ˜ë¦½ í•„ìš”")
        else:
            plan.append("âœ… ì–‘í˜¸: ì„¸ë¶€ ìµœì í™”ì— ì§‘ì¤‘")
        
        # ê³µí†µ ë¬¸ì œ ê¸°ë°˜
        if patterns['common_issues']:
            top_issue = patterns['common_issues'][0][0]
            count = patterns['common_issues'][0][1]
            plan.append(f"1. ìµœìš°ì„  ê°œì„ : {top_issue} ({count}ê°œ ë¬¸í•­)")
        
        # ìƒê´€ê´€ê³„ ê¸°ë°˜
        correlations = patterns['metric_correlations']
        weak_correlation = min(
            correlations.items(), 
            key=lambda x: x[1]['vs_overall']
        )
        
        if weak_correlation[1]['vs_overall'] < 0.5:
            plan.append(f"2. {weak_correlation[0]} ë©”íŠ¸ë¦­ ê°œì„  ì§‘ì¤‘")
        
        # ìµœì•… ë¬¸í•­ ê°œì„ 
        if patterns['worst_questions']:
            plan.append(f"3. í•˜ìœ„ {len(patterns['worst_questions'])}ê°œ ë¬¸í•­ ì§‘ì¤‘ ê°œì„ ")
        
        return plan