"""Report generator for evaluation results"""

import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional
import logging

logger = logging.getLogger(__name__)


class ReportGenerator:
    """ë³´ê³ ì„œ ìƒì„±ê¸°"""
    
    def __init__(self, template_dir: Optional[str] = None, use_llm_interpretation: bool = True):
        self.template_dir = Path(template_dir) if template_dir else None
        self.use_llm_interpretation = use_llm_interpretation
        
        # LLM í•´ì„ê¸° ì´ˆê¸°í™”
        if self.use_llm_interpretation:
            try:
                from ..stats.interpreter import StatisticalInterpreter
                self.interpreter = StatisticalInterpreter()
                logger.info("LLM interpreter initialized for reports")
            except Exception as e:
                logger.warning(f"Failed to initialize LLM interpreter: {e}")
                self.interpreter = None
        else:
            self.interpreter = None
    
    def generate_evaluation_report(
        self,
        run_id: str,
        results: Dict,
        environment: Dict,
        output_path: str
    ) -> str:
        """í‰ê°€ ë³´ê³ ì„œ ìƒì„±"""
        
        output_path = Path(output_path)
        # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Markdown ë³´ê³ ì„œ ìƒì„±
        report = self._create_markdown_report(run_id, results, environment)
        
        # íŒŒì¼ ì €ìž¥
        report_file = output_path / f"{run_id}_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        # JSON ê²°ê³¼ë„ ì €ìž¥
        json_file = output_path / f"{run_id}_results.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump({
                'run_id': run_id,
                'environment': environment,
                'metrics': results.get('metrics', {}),
                'timestamp': datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Report generated: {report_file}")
        return str(report_file)
    
    def _create_markdown_report(
        self,
        run_id: str,
        results: Dict,
        environment: Dict
    ) -> str:
        """Markdown í˜•ì‹ ë³´ê³ ì„œ ìƒì„±"""
        
        # í—¤ë”
        report = f"""# RAGTrace Evaluation Report

**Run ID**: {run_id}  
**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ðŸ“Š Evaluation Metrics

"""
        
        # ë©”íŠ¸ë¦­ í…Œì´ë¸”
        metrics = results.get('metrics', {})
        if metrics:
            report += "| Metric | Score |\n"
            report += "|--------|-------|\n"
            for metric, score in metrics.items():
                report += f"| {metric.replace('_', ' ').title()} | {score:.4f} |\n"
            
            # í‰ê·  ì ìˆ˜
            avg_score = sum(metrics.values()) / len(metrics)
            report += f"\n**Average Score**: {avg_score:.4f}\n"
            
            # LLM í•´ì„ ì¶”ê°€
            if self.interpreter:
                try:
                    interpretation = self.interpreter.interpret_metrics(
                        metrics, 
                        dataset_name=run_id
                    )
                    report += "\n### ðŸ¤– AI Analysis\n\n"
                    report += interpretation + "\n"
                except Exception as e:
                    logger.warning(f"Failed to generate LLM interpretation: {e}")
        
        # í™˜ê²½ ì •ë³´
        report += "\n## âš™ï¸ Environment Configuration\n\n"
        if environment:
            report += "| Parameter | Value |\n"
            report += "|-----------|-------|\n"
            for key, value in sorted(environment.items()):
                report += f"| {key} | {value} |\n"
        else:
            report += "_No environment configuration specified_\n"
        
        # ìƒì„¸ ê²°ê³¼ ìš”ì•½
        details = results.get('details', [])
        if details:
            report += f"\n## ðŸ“ Evaluation Details\n\n"
            report += f"- **Total Samples**: {len(details)}\n"
            
            # ë©”íŠ¸ë¦­ë³„ ë¶„í¬
            for metric in metrics.keys():
                values = [d.get(metric, 0) for d in details if metric in d]
                if values:
                    report += f"- **{metric.replace('_', ' ').title()}**: "
                    report += f"Min={min(values):.3f}, Max={max(values):.3f}, "
                    report += f"Avg={sum(values)/len(values):.3f}\n"
        
        # í‘¸í„°
        report += "\n---\n"
        report += "_Generated by RAGTrace Lite v2.0_\n"
        
        return report
    
    def generate_comparison_report(
        self,
        result,
        output_path: str
    ) -> str:
        """ë¹„êµ ë³´ê³ ì„œ ìƒì„±"""
        
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Markdown ë³´ê³ ì„œ ìƒì„±
        report = self._create_comparison_markdown(result)
        
        # íŒŒì¼ ì €ìž¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = output_path / f"comparison_{timestamp}.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"Comparison report generated: {report_file}")
        return str(report_file)
    
    def _create_comparison_markdown(self, result) -> str:
        """ë¹„êµ ë³´ê³ ì„œ Markdown ìƒì„±"""
        
        report = f"""# Window Comparison Report

**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**Metric**: {result.metric_name}

## ðŸ“Š Window Information

| Window | Period | Runs |
|--------|--------|------|
| A | {result.window_a['start']} to {result.window_a['end']} | {result.window_a['runs']} |
| B | {result.window_b['start']} to {result.window_b['end']} | {result.window_b['runs']} |

"""
        
        # ê²½ê³ 
        if result.warnings:
            report += "## âš ï¸ Warnings\n\n"
            for warning in result.warnings:
                report += f"- {warning}\n"
            report += "\n"
        
        # í†µê³„ ìš”ì•½
        report += "## ðŸ“ˆ Statistical Summary\n\n"
        report += "| Statistic | Window A | Window B |\n"
        report += "|-----------|----------|----------|\n"
        report += f"| Mean | {result.stats_a['mean']:.4f} | {result.stats_b['mean']:.4f} |\n"
        report += f"| Std Dev | {result.stats_a['std']:.4f} | {result.stats_b['std']:.4f} |\n"
        report += f"| Median | {result.stats_a['median']:.4f} | {result.stats_b['median']:.4f} |\n"
        report += f"| Min | {result.stats_a['min']:.4f} | {result.stats_b['min']:.4f} |\n"
        report += f"| Max | {result.stats_a['max']:.4f} | {result.stats_b['max']:.4f} |\n"
        
        # ë³€í™”ëŸ‰
        report += f"\n## ðŸ“Š Change Analysis\n\n"
        if result.improvement > 0:
            report += f"- **Improvement**: â†‘ {result.improvement:.4f} ({result.improvement_pct:+.1f}%)\n"
        elif result.improvement < 0:
            report += f"- **Decline**: â†“ {abs(result.improvement):.4f} ({result.improvement_pct:.1f}%)\n"
        else:
            report += "- **No Change**\n"
        
        # í†µê³„ ê²€ì •
        report += f"\n## ðŸ”¬ Statistical Test\n\n"
        report += f"- **Test Type**: {result.test_type}\n"
        
        if result.p_value is not None:
            report += f"- **P-value**: {result.p_value:.4f}\n"
            report += f"- **Significant**: {'Yes âœ…' if result.significant else 'No âŒ'}\n"
            
            if result.cohens_d:
                report += f"- **Effect Size**: {result.effect_size} (Cohen's d = {result.cohens_d:.3f})\n"
        
        # ì‹ ë¢°êµ¬ê°„
        report += f"\n## ðŸ“‰ Confidence Intervals (95%)\n\n"
        report += f"- **Window A**: [{result.confidence_interval_a[0]:.4f}, {result.confidence_interval_a[1]:.4f}]\n"
        report += f"- **Window B**: [{result.confidence_interval_b[0]:.4f}, {result.confidence_interval_b[1]:.4f}]\n"
        report += f"- **Overlapping**: {'Yes' if result.ci_overlap else 'No'}\n"
        
        # í•´ì„
        report += "\n## ðŸ’¡ Interpretation\n\n"
        
        # LLM ê¸°ë°˜ í•´ì„ ì‹œë„
        if self.interpreter:
            try:
                llm_interpretation = self.interpreter.interpret_comparison(result)
                report += "### ðŸ¤– AI Analysis\n\n"
                report += llm_interpretation + "\n\n"
                report += "### ðŸ“Š Statistical Details\n\n"
            except Exception as e:
                logger.warning(f"Failed to generate LLM interpretation: {e}")
        
        # ê¸°ë³¸ í†µê³„ í•´ì„
        if result.significant:
            report += f"The difference between Window A and Window B is **statistically significant** "
            report += f"(p = {result.p_value:.4f} < 0.05). "
            
            if result.effect_size == 'large':
                report += "The effect size is **large**, indicating a substantial practical difference.\n"
            elif result.effect_size == 'medium':
                report += "The effect size is **medium**, indicating a moderate practical difference.\n"
            elif result.effect_size == 'small':
                report += "The effect size is **small**, indicating a minor practical difference.\n"
            else:
                report += "The effect size is **negligible**, despite statistical significance.\n"
        else:
            report += "The difference between Window A and Window B is **not statistically significant** "
            if result.p_value:
                report += f"(p = {result.p_value:.4f} â‰¥ 0.05).\n"
            else:
                report += "due to insufficient data.\n"
        
        # í‘¸í„°
        report += "\n---\n"
        report += "_Generated by RAGTrace Lite v2.0_\n"
        
        return report