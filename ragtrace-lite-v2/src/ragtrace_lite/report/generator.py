"""Report generator for evaluation results"""

import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional
import logging

logger = logging.getLogger(__name__)


class ReportGenerator:
    """보고서 생성기"""
    
    def __init__(self, template_dir: Optional[str] = None, use_llm_interpretation: bool = True):
        self.template_dir = Path(template_dir) if template_dir else None
        self.use_llm_interpretation = use_llm_interpretation
        
        # LLM 해석기 초기화
        if self.use_llm_interpretation:
            try:
                from ..stats.interpreter import StatisticalInterpreter
                self.interpreter = StatisticalInterpreter()
                logger.info("LLM interpreter initialized for reports")
            except Exception as e:
                logger.warning(f"Failed to initialize LLM interpreter: {e}")
                self.interpreter = None
        else:
            self.interpreter = None
    
    def generate_evaluation_report(
        self,
        run_id: str,
        results: Dict,
        environment: Dict,
        output_path: str
    ) -> str:
        """평가 보고서 생성"""
        
        output_path = Path(output_path)
        # 디렉토리가 없으면 생성
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Markdown 보고서 생성
        report = self._create_markdown_report(run_id, results, environment)
        
        # 파일 저장
        report_file = output_path / f"{run_id}_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        # JSON 결과도 저장
        json_file = output_path / f"{run_id}_results.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump({
                'run_id': run_id,
                'environment': environment,
                'metrics': results.get('metrics', {}),
                'timestamp': datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Report generated: {report_file}")
        return str(report_file)
    
    def _create_markdown_report(
        self,
        run_id: str,
        results: Dict,
        environment: Dict
    ) -> str:
        """Markdown 형식 보고서 생성"""
        
        # 헤더
        report = f"""# RAGTrace Evaluation Report

**Run ID**: {run_id}  
**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## 📊 Evaluation Metrics

"""
        
        # 메트릭 테이블
        metrics = results.get('metrics', {})
        if metrics:
            report += "| Metric | Score |\n"
            report += "|--------|-------|\n"
            for metric, score in metrics.items():
                report += f"| {metric.replace('_', ' ').title()} | {score:.4f} |\n"
            
            # 평균 점수
            avg_score = sum(metrics.values()) / len(metrics)
            report += f"\n**Average Score**: {avg_score:.4f}\n"
            
            # LLM 해석 추가
            if self.interpreter:
                try:
                    interpretation = self.interpreter.interpret_metrics(
                        metrics, 
                        dataset_name=run_id
                    )
                    report += "\n### 🤖 AI Analysis\n\n"
                    report += interpretation + "\n"
                except Exception as e:
                    logger.warning(f"Failed to generate LLM interpretation: {e}")
        
        # 환경 정보
        report += "\n## ⚙️ Environment Configuration\n\n"
        if environment:
            report += "| Parameter | Value |\n"
            report += "|-----------|-------|\n"
            for key, value in sorted(environment.items()):
                report += f"| {key} | {value} |\n"
        else:
            report += "_No environment configuration specified_\n"
        
        # 상세 결과 요약
        details = results.get('details', [])
        if details:
            report += f"\n## 📝 Evaluation Details\n\n"
            report += f"- **Total Samples**: {len(details)}\n"
            
            # 메트릭별 분포
            for metric in metrics.keys():
                values = [d.get(metric, 0) for d in details if metric in d]
                if values:
                    report += f"- **{metric.replace('_', ' ').title()}**: "
                    report += f"Min={min(values):.3f}, Max={max(values):.3f}, "
                    report += f"Avg={sum(values)/len(values):.3f}\n"
        
        # 푸터
        report += "\n---\n"
        report += "_Generated by RAGTrace Lite v2.0_\n"
        
        return report
    
    def generate_comparison_report(
        self,
        result,
        output_path: str
    ) -> str:
        """비교 보고서 생성"""
        
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Markdown 보고서 생성
        report = self._create_comparison_markdown(result)
        
        # 파일 저장
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = output_path / f"comparison_{timestamp}.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"Comparison report generated: {report_file}")
        return str(report_file)
    
    def _create_comparison_markdown(self, result) -> str:
        """비교 보고서 Markdown 생성"""
        
        report = f"""# Window Comparison Report

**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**Metric**: {result.metric_name}

## 📊 Window Information

| Window | Period | Runs |
|--------|--------|------|
| A | {result.window_a['start']} to {result.window_a['end']} | {result.window_a['runs']} |
| B | {result.window_b['start']} to {result.window_b['end']} | {result.window_b['runs']} |

"""
        
        # 경고
        if result.warnings:
            report += "## ⚠️ Warnings\n\n"
            for warning in result.warnings:
                report += f"- {warning}\n"
            report += "\n"
        
        # 통계 요약
        report += "## 📈 Statistical Summary\n\n"
        report += "| Statistic | Window A | Window B |\n"
        report += "|-----------|----------|----------|\n"
        report += f"| Mean | {result.stats_a['mean']:.4f} | {result.stats_b['mean']:.4f} |\n"
        report += f"| Std Dev | {result.stats_a['std']:.4f} | {result.stats_b['std']:.4f} |\n"
        report += f"| Median | {result.stats_a['median']:.4f} | {result.stats_b['median']:.4f} |\n"
        report += f"| Min | {result.stats_a['min']:.4f} | {result.stats_b['min']:.4f} |\n"
        report += f"| Max | {result.stats_a['max']:.4f} | {result.stats_b['max']:.4f} |\n"
        
        # 변화량
        report += f"\n## 📊 Change Analysis\n\n"
        if result.improvement > 0:
            report += f"- **Improvement**: ↑ {result.improvement:.4f} ({result.improvement_pct:+.1f}%)\n"
        elif result.improvement < 0:
            report += f"- **Decline**: ↓ {abs(result.improvement):.4f} ({result.improvement_pct:.1f}%)\n"
        else:
            report += "- **No Change**\n"
        
        # 통계 검정
        report += f"\n## 🔬 Statistical Test\n\n"
        report += f"- **Test Type**: {result.test_type}\n"
        
        if result.p_value is not None:
            report += f"- **P-value**: {result.p_value:.4f}\n"
            report += f"- **Significant**: {'Yes ✅' if result.significant else 'No ❌'}\n"
            
            if result.cohens_d:
                report += f"- **Effect Size**: {result.effect_size} (Cohen's d = {result.cohens_d:.3f})\n"
        
        # 신뢰구간
        report += f"\n## 📉 Confidence Intervals (95%)\n\n"
        report += f"- **Window A**: [{result.confidence_interval_a[0]:.4f}, {result.confidence_interval_a[1]:.4f}]\n"
        report += f"- **Window B**: [{result.confidence_interval_b[0]:.4f}, {result.confidence_interval_b[1]:.4f}]\n"
        report += f"- **Overlapping**: {'Yes' if result.ci_overlap else 'No'}\n"
        
        # 해석
        report += "\n## 💡 Interpretation\n\n"
        
        # LLM 기반 해석 시도
        if self.interpreter:
            try:
                llm_interpretation = self.interpreter.interpret_comparison(result)
                report += "### 🤖 AI Analysis\n\n"
                report += llm_interpretation + "\n\n"
                report += "### 📊 Statistical Details\n\n"
            except Exception as e:
                logger.warning(f"Failed to generate LLM interpretation: {e}")
        
        # 기본 통계 해석
        if result.significant:
            report += f"The difference between Window A and Window B is **statistically significant** "
            report += f"(p = {result.p_value:.4f} < 0.05). "
            
            if result.effect_size == 'large':
                report += "The effect size is **large**, indicating a substantial practical difference.\n"
            elif result.effect_size == 'medium':
                report += "The effect size is **medium**, indicating a moderate practical difference.\n"
            elif result.effect_size == 'small':
                report += "The effect size is **small**, indicating a minor practical difference.\n"
            else:
                report += "The effect size is **negligible**, despite statistical significance.\n"
        else:
            report += "The difference between Window A and Window B is **not statistically significant** "
            if result.p_value:
                report += f"(p = {result.p_value:.4f} ≥ 0.05).\n"
            else:
                report += "due to insufficient data.\n"
        
        # 푸터
        report += "\n---\n"
        report += "_Generated by RAGTrace Lite v2.0_\n"
        
        return report