#!/usr/bin/env python3
"""
HCX ì‘ë‹µ ë””ë²„ê¹…ì„ ìœ„í•œ ì§ì ‘ í…ŒìŠ¤íŠ¸
"""
import os
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from ragtrace_lite.config_loader import load_config
from ragtrace_lite.llm_factory import create_llm


def test_metric_responses():
    """ê° ë©”íŠ¸ë¦­ì˜ ì‹¤ì œ ì‘ë‹µ í™•ì¸"""
    print("=" * 80)
    print("ğŸ” HCX ë©”íŠ¸ë¦­ë³„ ì‘ë‹µ ë””ë²„ê¹…")
    print("=" * 80)
    
    # ì„¤ì • ë° LLM ë¡œë“œ
    config = load_config()
    llm = create_llm(config)
    
    # í…ŒìŠ¤íŠ¸í•  í”„ë¡¬í”„íŠ¸ë“¤
    test_prompts = {
        "context_precision": """Given question, answer and context verify if the context was useful in arriving at the given answer. Give verdict as "1" if useful and "0" if not with json output.

Question: í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?
Answer: ì„œìš¸ì…ë‹ˆë‹¤.
Context: ì„œìš¸íŠ¹ë³„ì‹œëŠ” ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì´ì ìµœëŒ€ ë„ì‹œì…ë‹ˆë‹¤.

ì¤‘ìš”: ì»¨í…ìŠ¤íŠ¸ê°€ ë‹µë³€ì„ ìƒì„±í•˜ëŠ”ë° ìœ ìš©í–ˆëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.
ë°˜ë“œì‹œ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µí•˜ì„¸ìš”:
- "1" (ìœ ìš©í•¨)
- "0" (ìœ ìš©í•˜ì§€ ì•ŠìŒ)

ìˆ«ìë§Œ ë‹µí•˜ì„¸ìš”. ì„¤ëª…ì€ í•„ìš”ì—†ìŠµë‹ˆë‹¤.""",

        "context_recall": """Given a context, and an answer, analyze each sentence in the answer and classify if the sentence can be attributed to the given context or not.

Context: Pythonì˜ ë¦¬ìŠ¤íŠ¸ëŠ” ëŒ€ê´„í˜¸ []ë¥¼ ì‚¬ìš©í•˜ë©°, ë™ì ìœ¼ë¡œ í¬ê¸°ê°€ ë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
Answer: ë¦¬ìŠ¤íŠ¸ëŠ” ê°€ë³€ ê°ì²´ë¡œ ìš”ì†Œë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¦¬ìŠ¤íŠ¸ëŠ” ëŒ€ê´„í˜¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

ê° ë¬¸ì¥ì´ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì§€ì›ë˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”:
[1, 0, 1, 1, 0]

ê° ìˆ«ìëŠ”:
- 1: í•´ë‹¹ ë¬¸ì¥ì´ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì§€ì›ë¨
- 0: í•´ë‹¹ ë¬¸ì¥ì´ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŒ

ëŒ€ê´„í˜¸ ì•ˆì— ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ìˆ«ìë§Œ ë‹µí•˜ì„¸ìš”.""",

        "answer_correctness": """Given a ground truth and an answer statements, analyze each statement and classify them in one of the following categories:

Ground Truth: ë¦¬ìŠ¤íŠ¸ëŠ” ë³€ê²½ ê°€ëŠ¥í•˜ê³  íŠœí”Œì€ ë³€ê²½ ë¶ˆê°€ëŠ¥í•˜ë‹¤.
Answer: ë¦¬ìŠ¤íŠ¸ëŠ” ê°€ë³€ ê°ì²´ì´ê³  íŠœí”Œì€ ë¶ˆë³€ ê°ì²´ì…ë‹ˆë‹¤. ë¦¬ìŠ¤íŠ¸ëŠ” ëŒ€ê´„í˜¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

Ground truthì™€ answerë¥¼ ë¹„êµí•˜ì—¬ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:
{
  "TP": ["ì •ë‹µê³¼ ì¼ì¹˜í•˜ëŠ” ë¬¸ì¥1", "ë¬¸ì¥2"],
  "FP": ["ì •ë‹µì— ì—†ëŠ” ì˜ëª»ëœ ë¬¸ì¥3"],
  "FN": ["ì •ë‹µì—ëŠ” ìˆì§€ë§Œ ë‹µë³€ì— ì—†ëŠ” ë¬¸ì¥4"]
}

TP=True Positive, FP=False Positive, FN=False Negative"""
    }
    
    # ê° í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸
    for metric_name, prompt in test_prompts.items():
        print(f"\nğŸ“Š {metric_name}")
        print("-" * 60)
        print(f"í”„ë¡¬í”„íŠ¸:\n{prompt[:100]}...")
        
        try:
            # HCX í˜¸ì¶œ
            response = llm._call(prompt)
            print(f"\nì‘ë‹µ:\n{response}")
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                import json
                if response.strip().startswith('{') or response.strip().startswith('['):
                    parsed = json.loads(response)
                    print(f"\nâœ… JSON íŒŒì‹± ì„±ê³µ: {parsed}")
                else:
                    print(f"\nâš ï¸ JSONì´ ì•„ë‹Œ ì‘ë‹µ")
            except json.JSONDecodeError as e:
                print(f"\nâŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                
        except Exception as e:
            print(f"\nâŒ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
    
    print("\n" + "=" * 80)


if __name__ == "__main__":
    # API í‚¤ ì„¤ì •
    os.environ['CLOVA_STUDIO_API_KEY'] = "nv-d78e840d8f5c4e2faed883a52ea91375gmj8"
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_metric_responses()