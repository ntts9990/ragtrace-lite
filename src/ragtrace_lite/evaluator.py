"""
RAGTrace Lite Evaluator

RAGAS í‰ê°€ ì—”ì§„:
- 5ê°€ì§€ ë©”íŠ¸ë¦­ ì§€ì› (faithfulness, answer_relevancy, context_precision, context_recall, answer_correctness)
- ë°°ì¹˜ ì²˜ë¦¬ (batch_size í™œìš©)
- ì§„í–‰ë¥  í‘œì‹œ (tqdm)
- ë™ê¸°ì‹ í‰ê°€
"""

import pandas as pd
from typing import Dict, List, Any, Optional
from datasets import Dataset
from tqdm import tqdm

# RAGAS imports with fallback
try:
    from ragas import evaluate
    from ragas.metrics import (
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
        answer_correctness,
    )
    RAGAS_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  RAGAS import ì˜¤ë¥˜: {e}")
    RAGAS_AVAILABLE = False

from .config_loader import Config
from .llm_factory import create_llm


class RagasEvaluator:
    """RAGTrace Lite RAGAS í‰ê°€ í´ë˜ìŠ¤"""
    
    # ë©”íŠ¸ë¦­ ë§¤í•‘
    METRIC_MAP = {
        "faithfulness": "faithfulness",
        "answer_relevancy": "answer_relevancy", 
        "context_precision": "context_precision",
        "context_recall": "context_recall",
        "answer_correctness": "answer_correctness",
    } if RAGAS_AVAILABLE else {}
    
    def __init__(self, config: Config, llm=None):
        """
        RAGAS í‰ê°€ì ì´ˆê¸°í™”
        
        Args:
            config: RAGTrace Lite ì„¤ì •
            llm: ì‚¬ì „ ìƒì„±ëœ LLM ì¸ìŠ¤í„´ìŠ¤ (ì˜µì…˜)
            
        Raises:
            ImportError: RAGASê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš°
            ValueError: ì„¤ì •ì´ ì˜¬ë°”ë¥´ì§€ ì•Šì€ ê²½ìš°
        """
        if not RAGAS_AVAILABLE:
            raise ImportError("RAGAS ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install ragas' ì‹¤í–‰í•˜ì„¸ìš”.")
            
        self.config = config
        
        # LLM ì¸ìŠ¤í„´ìŠ¤ ì„¤ì •
        if llm:
            print(f"ğŸ¤– ì™¸ë¶€ LLM ì‚¬ìš©: {config.llm.provider}")
            self.llm = llm
        else:
            print(f"ğŸ¤– ìƒˆ LLM ìƒì„±: {config.llm.provider}")
            self.llm = create_llm(config)
        
        # ì„ë² ë”© ëª¨ë¸ ì„¤ì • (HuggingFace ë¬´ë£Œ ëª¨ë¸ ì‚¬ìš©)
        try:
            from langchain_community.embeddings import HuggingFaceEmbeddings
            self.embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2"
            )
            print("âœ… HuggingFace ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except ImportError:
            print("âš ï¸  HuggingFace ì„ë² ë”© ì‚¬ìš© ë¶ˆê°€, ê¸°ë³¸ ì„ë² ë”© ì‚¬ìš©")
            self.embeddings = None
        
        # í‰ê°€ ë©”íŠ¸ë¦­ ì„¤ì •
        self.metrics = self._setup_metrics()
        
        print(f"âœ… í‰ê°€ì ì´ˆê¸°í™” ì™„ë£Œ: {len(self.metrics)}ê°œ ë©”íŠ¸ë¦­")
    
    def _setup_metrics(self) -> List[Any]:
        """í‰ê°€ ë©”íŠ¸ë¦­ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
        metrics = []
        
        print("ğŸ”§ ë©”íŠ¸ë¦­ ì„¤ì • ì¤‘...")
        
        for metric_name in self.config.evaluation.metrics:
            try:
                if metric_name == "faithfulness":
                    metric = faithfulness
                    metric.llm = self.llm
                    metrics.append(metric)
                    print(f"  âœ… {metric_name} (LLM ê¸°ë°˜)")
                    
                elif metric_name == "answer_relevancy":
                    metric = answer_relevancy
                    metric.llm = self.llm
                    if self.embeddings:
                        metric.embeddings = self.embeddings
                    metrics.append(metric)
                    print(f"  âœ… {metric_name} (LLM + ì„ë² ë”© ê¸°ë°˜)")
                    
                elif metric_name == "context_precision":
                    metric = context_precision
                    metric.llm = self.llm
                    metrics.append(metric)
                    print(f"  âœ… {metric_name} (LLM ê¸°ë°˜)")
                    
                elif metric_name == "context_recall":
                    metric = context_recall
                    metric.llm = self.llm
                    metrics.append(metric)
                    print(f"  âœ… {metric_name} (LLM ê¸°ë°˜)")
                    
                elif metric_name == "answer_correctness":
                    metric = answer_correctness
                    metric.llm = self.llm
                    metrics.append(metric)
                    print(f"  âœ… {metric_name} (LLM ê¸°ë°˜)")
                    
                else:
                    print(f"  âš ï¸  ì•Œ ìˆ˜ ì—†ëŠ” ë©”íŠ¸ë¦­: {metric_name}")
                    
            except Exception as e:
                print(f"  âŒ {metric_name} ì„¤ì • ì‹¤íŒ¨: {e}")
        
        if not metrics:
            raise ValueError("ì„¤ì •ëœ ë©”íŠ¸ë¦­ì´ ì—†ìŠµë‹ˆë‹¤")
            
        return metrics
    
    def evaluate(self, dataset: Dataset) -> pd.DataFrame:
        """
        ë°ì´í„°ì…‹ì— ëŒ€í•´ RAGAS í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            dataset: í‰ê°€í•  RAGAS Dataset
            
        Returns:
            pd.DataFrame: í‰ê°€ ê²°ê³¼ (ê° í•­ëª©ë³„ ë©”íŠ¸ë¦­ ì ìˆ˜)
            
        Raises:
            ValueError: ë°ì´í„°ì…‹ì´ ì˜¬ë°”ë¥´ì§€ ì•Šì€ ê²½ìš°
            Exception: í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš°
        """
        print(f"\nğŸš€ RAGAS í‰ê°€ ì‹œì‘")
        print(f"   - ë°ì´í„° ìˆ˜: {len(dataset)}ê°œ")
        print(f"   - ë©”íŠ¸ë¦­: {len(self.metrics)}ê°œ")
        print(f"   - LLM: {self.config.llm.provider}")
        print(f"   - ë°°ì¹˜ í¬ê¸°: {self.config.evaluation.batch_size}")
        
        # ë°ì´í„°ì…‹ ê²€ì¦
        self._validate_dataset(dataset)
        
        try:
            # RAGAS evaluate í˜¸ì¶œ
            print("\nğŸ“Š í‰ê°€ ì§„í–‰ ì¤‘...")
            
            # RAGASëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ì§„í–‰ë¥ ì„ í‘œì‹œí•˜ë¯€ë¡œ ë³„ë„ tqdm ë¶ˆí•„ìš”
            result = evaluate(
                dataset=dataset,
                metrics=self.metrics,
                llm=self.llm,
                embeddings=self.embeddings,
                raise_exceptions=not self.config.evaluation.raise_exceptions,
                show_progress=self.config.evaluation.show_progress,
            )
            
            print("âœ… í‰ê°€ ì™„ë£Œ!")
            
            # ê²°ê³¼ë¥¼ pandas DataFrameìœ¼ë¡œ ë³€í™˜
            results_df = result.to_pandas()
            
            # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            self._print_evaluation_summary(results_df)
            
            return results_df
            
        except Exception as e:
            print(f"âŒ í‰ê°€ ì‹¤íŒ¨: {e}")
            raise Exception(f"RAGAS í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def _validate_dataset(self, dataset: Dataset) -> None:
        """í‰ê°€ìš© ë°ì´í„°ì…‹ì„ ê²€ì¦í•©ë‹ˆë‹¤."""
        
        # ê¸°ë³¸ ê²€ì¦
        if len(dataset) == 0:
            raise ValueError("ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['question', 'answer', 'contexts']
        missing_columns = [col for col in required_columns if col not in dataset.column_names]
        
        if missing_columns:
            raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing_columns}")
        
        # ground_truths ì»¬ëŸ¼ í™•ì¸ (answer_correctness, context_recallìš©)
        if ('answer_correctness' in self.config.evaluation.metrics or 
            'context_recall' in self.config.evaluation.metrics):
            if 'ground_truths' not in dataset.column_names:
                print("âš ï¸  'ground_truths' ì»¬ëŸ¼ì´ ì—†ì–´ answer_correctness/context_recall í‰ê°€ê°€ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        
        # reference ì»¬ëŸ¼ í™•ì¸ (context_precisionìš©)
        if 'context_precision' in self.config.evaluation.metrics:
            if 'reference' not in dataset.column_names:
                print("âš ï¸  'reference' ì»¬ëŸ¼ì´ ì—†ì–´ context_precision í‰ê°€ë¥¼ ìœ„í•´ ground_truthsë¥¼ referenceë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤")
                # ground_truthsë¥¼ referenceë¡œ ë³µì‚¬
                if 'ground_truths' in dataset.column_names:
                    # Dataset ìˆ˜ì •ì€ ê¹Œë‹¤ë¡œìš°ë¯€ë¡œ ì„ì‹œ í•´ê²°ì±…
                    pass
        
        print(f"âœ… ë°ì´í„°ì…‹ ê²€ì¦ ì™„ë£Œ")
    
    def _print_evaluation_summary(self, results_df: pd.DataFrame) -> None:
        """í‰ê°€ ê²°ê³¼ ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
        
        print(f"\nğŸ“ˆ í‰ê°€ ê²°ê³¼ ìš”ì•½:")
        print(f"{'='*50}")
        
        # ê° ë©”íŠ¸ë¦­ë³„ í‰ê·  ì ìˆ˜
        for metric_name in self.config.evaluation.metrics:
            if metric_name in results_df.columns:
                scores = results_df[metric_name].dropna()
                if len(scores) > 0:
                    avg_score = scores.mean()
                    min_score = scores.min()
                    max_score = scores.max()
                    
                    print(f"{metric_name:20}: {avg_score:.4f} (ë²”ìœ„: {min_score:.4f}-{max_score:.4f})")
                else:
                    print(f"{metric_name:20}: ë°ì´í„° ì—†ìŒ")
        
        # ì „ì²´ í‰ê·  (RAGAS Score)
        metric_columns = [col for col in results_df.columns if col in self.config.evaluation.metrics]
        if metric_columns:
            overall_scores = results_df[metric_columns].mean(axis=1)
            overall_avg = overall_scores.mean()
            print(f"{'='*50}")
            print(f"{'ì „ì²´ í‰ê·  (RAGAS Score)':20}: {overall_avg:.4f}")
        
        print(f"{'='*50}")
    
    def get_detailed_results(self, results_df: pd.DataFrame) -> Dict[str, Any]:
        """ìƒì„¸í•œ í‰ê°€ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        
        detailed_results = {
            'summary': {},
            'by_metric': {},
            'by_item': {},
            'statistics': {}
        }
        
        # ë©”íŠ¸ë¦­ë³„ í†µê³„
        for metric_name in self.config.evaluation.metrics:
            if metric_name in results_df.columns:
                scores = results_df[metric_name].dropna()
                if len(scores) > 0:
                    detailed_results['by_metric'][metric_name] = {
                        'mean': float(scores.mean()),
                        'std': float(scores.std()),
                        'min': float(scores.min()),
                        'max': float(scores.max()),
                        'count': len(scores)
                    }
        
        # ì „ì²´ í†µê³„
        metric_columns = [col for col in results_df.columns if col in self.config.evaluation.metrics]
        if metric_columns:
            overall_scores = results_df[metric_columns].mean(axis=1)
            detailed_results['summary'] = {
                'ragas_score': float(overall_scores.mean()),
                'total_items': len(results_df),
                'evaluated_metrics': len(metric_columns)
            }
        
        return detailed_results


def test_evaluator():
    """í‰ê°€ì í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸ§ª RagasEvaluator í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    try:
        # ì„¤ì • ë° ë°ì´í„° ë¡œë“œ
        from .config_loader import load_config
        from .data_processor import DataProcessor
        
        config = load_config()
        processor = DataProcessor()
        dataset = processor.load_and_prepare_data("data/input/sample.json")
        
        print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(dataset)}ê°œ í•­ëª©")
        
        # í‰ê°€ì ìƒì„±
        evaluator = RagasEvaluator(config)
        
        # í‰ê°€ ìˆ˜í–‰ (ì‘ì€ ë°ì´í„°ì…‹ì´ë¯€ë¡œ ë¹ ë¦„)
        results_df = evaluator.evaluate(dataset)
        
        print(f"\nâœ… í‰ê°€ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        print(f"   - ê²°ê³¼ DataFrame í¬ê¸°: {results_df.shape}")
        print(f"   - ì»¬ëŸ¼: {list(results_df.columns)}")
        
        # ìƒì„¸ ê²°ê³¼
        detailed = evaluator.get_detailed_results(results_df)
        print(f"   - RAGAS Score: {detailed['summary'].get('ragas_score', 'N/A'):.4f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ í‰ê°€ì í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


if __name__ == "__main__":
    test_evaluator()