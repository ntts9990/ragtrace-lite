"""
RAGTrace Lite LLM Factory (ë¹„ë™ê¸° ë²„ì „)

ë©”ì¸ RAGTrace ì–´ëŒ‘í„° ìœ„ì„ íŒ¨í„´ ì‚¬ìš©:
- ë³„ë„ ì–´ëŒ‘í„° í´ë˜ìŠ¤ + LangChain ë˜í¼
- Pydantic í•„ë“œ ë¬¸ì œ íšŒí”¼
- RAGAS ì™„ì „ í˜¸í™˜
"""

import asyncio
import json
import time
import uuid
import aiohttp
import google.generativeai as genai
from typing import List, Dict, Any, Optional
from langchain_core.language_models.llms import LLM
from langchain_core.outputs import Generation, LLMResult
from langchain_core.callbacks import AsyncCallbackManagerForLLMRun, CallbackManagerForLLMRun
from langchain_core.prompt_values import StringPromptValue

from .config_loader import Config


class GeminiAdapter:
    """Gemini API ì–´ëŒ‘í„° (ìˆœìˆ˜ Python í´ë˜ìŠ¤)"""
    
    def __init__(self, api_key: str, model_name: str = "gemini-2.5-flash"):
        self.api_key = api_key
        self.model_name = model_name
        
        # Gemini API ì„¤ì •
        genai.configure(api_key=api_key)
        self.gemini_model = genai.GenerativeModel(model_name)
        
        print(f"ğŸ¤– Gemini ì–´ëŒ‘í„° ì´ˆê¸°í™”: {model_name}")
    
    async def agenerate_answer(self, prompt: str, **kwargs) -> str:
        """Gemini API ë¹„ë™ê¸° í˜¸ì¶œ"""
        try:
            # ìƒì„± ì„¤ì •
            generation_config = {
                'temperature': kwargs.get('temperature', 0.1),
                'max_output_tokens': kwargs.get('max_tokens', 8192),  # RAGASë¥¼ ìœ„í•´ ì¦ê°€
                'top_p': kwargs.get('top_p', 0.95),
            }
            
            # ì•ˆì „ ì„¤ì •
            safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]
            
            # ë¹„ë™ê¸° ì‹¤í–‰
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.gemini_model.generate_content(
                    prompt,
                    generation_config=generation_config,
                    safety_settings=safety_settings
                )
            )
            
            # ì‘ë‹µ ì²˜ë¦¬
            if response.candidates:
                candidate = response.candidates[0]
                # finish_reason í™•ì¸ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
                if candidate.finish_reason == 1:  # STOP
                    return response.text if response.text else "ì‘ë‹µì´ ìƒì„±ë˜ì—ˆìœ¼ë‚˜ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
                elif candidate.finish_reason == 2:  # MAX_TOKENS
                    # ë¶€ë¶„ ì‘ë‹µì´ë¼ë„ ìˆìœ¼ë©´ ë°˜í™˜
                    if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                        text_parts = []
                        for part in candidate.content.parts:
                            if hasattr(part, 'text'):
                                text_parts.append(part.text)
                        if text_parts:
                            return ''.join(text_parts) + " [ìµœëŒ€ í† í° ìˆ˜ ë„ë‹¬]"
                    return "ìµœëŒ€ í† í° ìˆ˜ì— ë„ë‹¬í–ˆìœ¼ë‚˜ ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤."
                elif candidate.finish_reason == 3:  # SAFETY
                    return "ì•ˆì „ í•„í„°ì— ì˜í•´ ì°¨ë‹¨ëœ ì‘ë‹µì…ë‹ˆë‹¤."
                else:
                    # ê¸°íƒ€ ê²½ìš°ì—ë„ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
                    try:
                        return response.text
                    except:
                        if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                            text_parts = []
                            for part in candidate.content.parts:
                                if hasattr(part, 'text'):
                                    text_parts.append(part.text)
                            if text_parts:
                                return ''.join(text_parts)
                        return f"ì‘ë‹µ ì¶”ì¶œ ì‹¤íŒ¨ (finish_reason: {candidate.finish_reason})"
            else:
                return "ì‘ë‹µ í›„ë³´ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                
        except Exception as e:
            print(f"âŒ Gemini API ì˜¤ë¥˜: {e}")
            return f"Gemini API ì˜¤ë¥˜: {str(e)}"
    
    def generate_answer(self, prompt: str, **kwargs) -> str:
        """ë™ê¸° í˜¸ì¶œ (ë¹„ë™ê¸°ë¥¼ ë™ê¸°ë¡œ ë˜í•‘)"""
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìˆìœ¼ë©´ executor ì‚¬ìš©
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, self.agenerate_answer(prompt, **kwargs))
                    return future.result()
            else:
                return asyncio.run(self.agenerate_answer(prompt, **kwargs))
        except Exception as e:
            print(f"âŒ Gemini ë™ê¸° í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return f"Error: {str(e)}"


class HcxAdapter:
    """HCX API ì–´ëŒ‘í„° (ìˆœìˆ˜ Python í´ë˜ìŠ¤)"""
    
    # í´ë˜ìŠ¤ ë³€ìˆ˜ë¡œ ë§ˆì§€ë§‰ ìš”ì²­ ì‹œê°„ ì €ì¥
    _last_request_time = 0
    _min_request_interval = 2.0  # ìµœì†Œ 2ì´ˆ ê°„ê²© (HCX API ì œí•œ ëŒ€ì‘)
    
    def __init__(self, api_key: str, model_name: str = "HCX-005"):
        # API í‚¤ ê²€ì¦
        if not api_key:
            raise ValueError("CLOVA_STUDIO_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        if not api_key.startswith("nv-"):
            raise ValueError("CLOVA_STUDIO_API_KEYëŠ” 'nv-'ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.")
            
        self.api_key = api_key
        self.model_name = model_name
        self.api_url = f"https://clovastudio.stream.ntruss.com/testapp/v3/chat-completions/{model_name}"
        
        # ë™ì  ìš”ì²­ ID ìƒì„±
        request_id = str(uuid.uuid4())
        
        self.headers = {
            "Authorization": f"Bearer {api_key}",  # ì¤‘ìš”: Bearer í† í° ì¶”ê°€
            "X-NCP-CLOVASTUDIO-API-KEY": api_key,
            "X-NCP-APIGW-API-KEY": api_key,
            "X-NCP-CLOVASTUDIO-REQUEST-ID": request_id,
            "Content-Type": "application/json",
            "Accept": "application/json"
        }
        
        print(f"ğŸ¤– HCX ì–´ëŒ‘í„° ì´ˆê¸°í™”: {model_name}")
    
    async def agenerate_answer(self, prompt: str, **kwargs) -> str:
        """HCX API ë¹„ë™ê¸° í˜¸ì¶œ"""
        try:
            # Rate limiting - ìš”ì²­ ê°„ ìµœì†Œ ê°„ê²© ìœ ì§€
            current_time = time.time()
            time_since_last = current_time - HcxAdapter._last_request_time
            if time_since_last < HcxAdapter._min_request_interval:
                wait_time = HcxAdapter._min_request_interval - time_since_last
                await asyncio.sleep(wait_time)
            
            HcxAdapter._last_request_time = time.time()
            
            payload = {
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "topP": kwargs.get('top_p', 0.8),
                "topK": kwargs.get('top_k', 0),
                "maxTokens": kwargs.get('max_tokens', 1000),
                "temperature": kwargs.get('temperature', 0.5),
                "repetitionPenalty": kwargs.get('repetition_penalty', 1.1),
                "stop": [],
                "includeAiFilters": True,
                "seed": 0
            }
            
            # ê° ìš”ì²­ë§ˆë‹¤ ìƒˆë¡œìš´ ìš”ì²­ ID ìƒì„±
            headers = self.headers.copy()
            headers["X-NCP-CLOVASTUDIO-REQUEST-ID"] = str(uuid.uuid4())
            
            # aiohttp ë¹„ë™ê¸° ìš”ì²­
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.api_url,
                    headers=headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    
                    if response.status == 200:
                        result = await response.json()
                        
                        if 'result' in result and 'message' in result['result']:
                            content = result['result']['message']['content']
                            return content if content else "HCX APIì—ì„œ ë¹ˆ ì‘ë‹µì„ ë°›ì•˜ìŠµë‹ˆë‹¤."
                        else:
                            return f"HCX API ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜: {result}"
                    else:
                        error_text = await response.text()
                        print(f"âŒ HCX API ì˜¤ë¥˜ {response.status}: {error_text}")
                        return f"HCX API ì˜¤ë¥˜ ({response.status}): {error_text}"
                        
        except asyncio.TimeoutError:
            print("âŒ HCX API íƒ€ì„ì•„ì›ƒ")
            return "HCX API ìš”ì²­ íƒ€ì„ì•„ì›ƒ"
        except Exception as e:
            print(f"âŒ HCX API ì˜¤ë¥˜: {e}")
            return f"HCX API ì˜¤ë¥˜: {str(e)}"
    
    def generate_answer(self, prompt: str, **kwargs) -> str:
        """ë™ê¸° í˜¸ì¶œ (ë¹„ë™ê¸°ë¥¼ ë™ê¸°ë¡œ ë˜í•‘)"""
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìˆìœ¼ë©´ executor ì‚¬ìš©
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, self.agenerate_answer(prompt, **kwargs))
                    return future.result()
            else:
                return asyncio.run(self.agenerate_answer(prompt, **kwargs))
        except Exception as e:
            print(f"âŒ HCX ë™ê¸° í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return f"Error: {str(e)}"


class LLMAdapterWrapper(LLM):
    """LLM ì–´ëŒ‘í„° ë˜í¼ - ë©”ì¸ RAGTrace HcxLangChainCompat êµ¬ì¡° ì°¸ì¡°"""
    
    adapter: Any = None
    model: str = None
    
    def __init__(self, adapter, **kwargs):
        super().__init__(**kwargs)
        self.adapter = adapter
        self.model = adapter.model_name
    
    @property
    def _llm_type(self) -> str:
        return "ragtrace_lite_adapter"
    
    def set_run_config(self, run_config):
        """RAGAS RunConfig ì„¤ì • - ë¬´ì‹œ"""
        # ìì²´ ì„¤ì •ì„ ì‚¬ìš©í•˜ë¯€ë¡œ RunConfigëŠ” ë¬´ì‹œ
        pass
    
    def _call(self, prompt: str, stop: Optional[List[str]] = None,
              run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any) -> str:
        """ë™ê¸° í˜¸ì¶œ"""
        return self.adapter.generate_answer(prompt, **kwargs)
    
    async def _acall(self, prompt: str, stop: Optional[List[str]] = None,
                     run_manager: Optional[AsyncCallbackManagerForLLMRun] = None, **kwargs: Any) -> str:
        """ë¹„ë™ê¸° í˜¸ì¶œ"""
        return await self.adapter.agenerate_answer(prompt, **kwargs)
    
    def generate(self, prompts: List[str | StringPromptValue], **kwargs: Any):
        """RAGAS í˜¸í™˜ generate - ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ê°ì§€"""
        # RAGAS ë²„ê·¸ ìš°íšŒ: RAGASê°€ await llm.generate()ë¥¼ í˜¸ì¶œí•˜ë¯€ë¡œ
        # ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ì—ì„œëŠ” ì½”ë£¨í‹´ì„ ë°˜í™˜í•´ì•¼ í•¨
        try:
            # ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ í™•ì¸
            loop = asyncio.get_running_loop()
            # ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì‹¤í–‰ ì¤‘ì´ë©´ agenerate ë°˜í™˜
            return self.agenerate(prompts, **kwargs)
        except RuntimeError:
            # ë™ê¸° ì»¨í…ìŠ¤íŠ¸ì—ì„œëŠ” ì¼ë°˜ LLMResult ë°˜í™˜
            if not isinstance(prompts, list):
                prompts = [prompts]
            
            generations = []
            for prompt in prompts:
                # í”„ë¡¬í”„íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                if hasattr(prompt, 'to_string'):
                    prompt_str = prompt.to_string()
                else:
                    prompt_str = str(prompt)
                
                # ì–´ëŒ‘í„° í˜¸ì¶œ
                response = self._call(prompt_str, **kwargs)
                
                # Generation ê°ì²´ ìƒì„±
                generation = Generation(text=response)
                generations.append([generation])
            
            return LLMResult(generations=generations)
    
    def agenerate(self, prompts: List[str | StringPromptValue], **kwargs: Any):
        """ë¹„ë™ê¸° generate - RAGAS í˜¸í™˜ (ì½”ë£¨í‹´ ë°˜í™˜)"""
        async def _agenerate():
            if not isinstance(prompts, list):
                prompts_list = [prompts]
            else:
                prompts_list = prompts
            
            generations = []
            
            # ëª¨ë“  í”„ë¡¬í”„íŠ¸ë¥¼ ë™ì‹œì— ì²˜ë¦¬
            tasks = []
            for prompt in prompts_list:
                if hasattr(prompt, 'to_string'):
                    prompt_str = prompt.to_string()
                else:
                    prompt_str = str(prompt)
                tasks.append(self._acall(prompt_str, **kwargs))
            
            responses = await asyncio.gather(*tasks)
            
            for response in responses:
                generation = Generation(text=response)
                generations.append([generation])
            
            return LLMResult(generations=generations)
        
        # ì½”ë£¨í‹´ ê°ì²´ ë°˜í™˜ (await ê°€ëŠ¥)
        return _agenerate()


def create_llm(config: Config) -> LLM:
    """RAGAS í˜¸í™˜ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    provider = config.llm.provider.lower()
    
    try:
        if provider == 'gemini':
            if not config.llm.api_key:
                raise ValueError("Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            model_name = config.llm.model_name or "gemini-2.5-flash"
            adapter = GeminiAdapter(
                api_key=config.llm.api_key,
                model_name=model_name
            )
            return LLMAdapterWrapper(adapter)
            
        elif provider == 'hcx':
            if not config.llm.api_key:
                raise ValueError("HCX API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            model_name = config.llm.model_name or "HCX-005"
            adapter = HcxAdapter(
                api_key=config.llm.api_key,
                model_name=model_name
            )
            return LLMAdapterWrapper(adapter)
            
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ì œê³µì: {provider}")
            
    except Exception as e:
        raise Exception(f"LLM ì´ˆê¸°í™” ì‹¤íŒ¨ ({provider}): {str(e)}")


async def test_llm_connection_async(llm: LLM, provider: str) -> bool:
    """LLM ë¹„ë™ê¸° ì—°ê²° í…ŒìŠ¤íŠ¸"""
    test_prompt = "Hello, this is a test. Please respond with 'OK'."
    
    try:
        print(f"ğŸ”„ {provider.upper()} LLM ë¹„ë™ê¸° ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
        response = await llm._acall(test_prompt)
        print(f"âœ… {provider.upper()} LLM ì—°ê²° ì„±ê³µ")
        print(f"í…ŒìŠ¤íŠ¸ ì‘ë‹µ: {response[:100]}...")
        return True
        
    except Exception as e:
        print(f"âŒ {provider.upper()} LLM ì—°ê²° ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def test_llm_connection(llm: LLM, provider: str) -> bool:
    """LLM ì—°ê²° í…ŒìŠ¤íŠ¸ (ë™ê¸° ë˜í¼)"""
    try:
        return asyncio.run(test_llm_connection_async(llm, provider))
    except Exception as e:
        print(f"âŒ {provider.upper()} LLM ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return False


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    import asyncio
    from .config_loader import load_config
    
    async def test_main():
        try:
            config = load_config()
            print(f"ì„¤ì • ë¡œë“œ ì™„ë£Œ: {config.llm.provider}")
            
            llm = create_llm(config)
            print("LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ")
            
            success = await test_llm_connection_async(llm, config.llm.provider)
            
            if success:
                print("âœ… LLM Factory ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            else:
                print("âŒ LLM Factory ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                
        except Exception as e:
            print(f"âŒ LLM Factory í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
    
    asyncio.run(test_main())