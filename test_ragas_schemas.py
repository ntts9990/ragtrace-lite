#!/usr/bin/env python3
"""
RAGASì˜ ì •í™•í•œ ìŠ¤í‚¤ë§ˆ í™•ì¸
"""
import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

# RAGAS ë©”íŠ¸ë¦­ ì„í¬íŠ¸
from ragas.metrics import (
    context_precision,
    context_recall,
    answer_correctness,
)

# Pydantic ëª¨ë¸ í™•ì¸
print("=" * 80)
print("RAGAS ë©”íŠ¸ë¦­ë³„ Pydantic ìŠ¤í‚¤ë§ˆ ë¶„ì„")
print("=" * 80)

# Context Precision
print("\nğŸ“Š Context Precision")
print("-" * 60)
if hasattr(context_precision, 'context_precision_prompt'):
    prompt = context_precision.context_precision_prompt
    if hasattr(prompt, 'output_type'):
        print(f"Output Type: {prompt.output_type}")
        if hasattr(prompt.output_type, '__fields__'):
            print("Fields:")
            for field_name, field_info in prompt.output_type.__fields__.items():
                print(f"  - {field_name}: {field_info.annotation}")

# Context Recall
print("\nğŸ“Š Context Recall")  
print("-" * 60)
if hasattr(context_recall, 'context_recall_prompt'):
    prompt = context_recall.context_recall_prompt
    if hasattr(prompt, 'output_type'):
        print(f"Output Type: {prompt.output_type}")
        if hasattr(prompt.output_type, '__fields__'):
            print("Fields:")
            for field_name, field_info in prompt.output_type.__fields__.items():
                print(f"  - {field_name}: {field_info.annotation}")

# Answer Correctness
print("\nğŸ“Š Answer Correctness")
print("-" * 60)
if hasattr(answer_correctness, 'correctness_prompt'):
    prompt = answer_correctness.correctness_prompt
    if hasattr(prompt, 'output_type'):
        print(f"Output Type: {prompt.output_type}")
        if hasattr(prompt.output_type, '__fields__'):
            print("Fields:")
            for field_name, field_info in prompt.output_type.__fields__.items():
                print(f"  - {field_name}: {field_info.annotation}")

# ì‹¤ì œ í”„ë¡¬í”„íŠ¸ ì˜ˆì œ ìƒì„±
print("\n\nğŸ“ í”„ë¡¬í”„íŠ¸ ì¶œë ¥ íƒ€ì… ì˜ˆì œ:")
print("=" * 80)

# ê° ë©”íŠ¸ë¦­ì˜ í”„ë¡¬í”„íŠ¸ ê°ì²´ í™•ì¸
for metric_name, metric in [
    ("context_precision", context_precision),
    ("context_recall", context_recall),
    ("answer_correctness", answer_correctness)
]:
    print(f"\n{metric_name}:")
    
    # í”„ë¡¬í”„íŠ¸ ê°ì²´ ì°¾ê¸°
    prompt_obj = None
    for attr in dir(metric):
        if 'prompt' in attr and not attr.startswith('_'):
            obj = getattr(metric, attr)
            if hasattr(obj, 'output_type'):
                prompt_obj = obj
                break
    
    if prompt_obj and hasattr(prompt_obj, 'output_type'):
        try:
            # ì˜ˆì œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            if prompt_obj.output_type.__name__ == 'ContextPrecisionVerification':
                example = {"reason": "ì»¨í…ìŠ¤íŠ¸ê°€ ë‹µë³€ì— ë„ì›€ì´ ë¨", "verdict": 1}
            elif prompt_obj.output_type.__name__ == 'ContextRecallClassifications':
                example = {"classifications": [
                    {"statement": "ë¬¸ì¥1", "reason": "ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì§€ì›ë¨", "attributed": 1}
                ]}
            elif prompt_obj.output_type.__name__ == 'CorrectnessClassifications':
                example = {
                    "TP": [{"statement": "ì˜¬ë°”ë¥¸ ë¬¸ì¥", "reason": "ì •ë‹µê³¼ ì¼ì¹˜"}],
                    "FP": [{"statement": "í‹€ë¦° ë¬¸ì¥", "reason": "ì •ë‹µì— ì—†ìŒ"}],
                    "FN": []
                }
            else:
                example = "Unknown type"
            
            print(f"  Expected format: {example}")
        except Exception as e:
            print(f"  Error creating example: {e}")

print("\n" + "=" * 80)